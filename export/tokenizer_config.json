{
  "tokenizer_class": "SentencePieceTokenizer",
  "model_type": "muomni",
  "vocab_size": 32000,
  "model_file": "tokenizer.model",
  "bos_token": "<BOS>",
  "eos_token": "<EOS>",
  "unk_token": "<UNK>",
  "pad_token": "<PAD>",
  "clean_up_spaces": true
}