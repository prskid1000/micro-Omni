# Chapter 33: Codebase Structure Guide

[â† Previous: Inference Pipeline](32-inference-pipeline.md) | [Back to Index](00-INDEX.md) | [Next: Configuration Files â†’](34-configuration-files.md)

---

## ğŸ“‚ Directory Structure

```
Î¼Omni/
â”œâ”€â”€ omni/                      # Core modules
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ thinker.py            # Decoder-only LLM (20.32M params)
â”‚   â”œâ”€â”€ audio_encoder.py      # AuT-Tiny (2.05M params)
â”‚   â”œâ”€â”€ vision_encoder.py     # ViT-Tiny (914K params)
â”‚   â”œâ”€â”€ talker.py             # Speech generator (2.24M params)
â”‚   â”œâ”€â”€ codec.py              # RVQ + Griffin-Lim vocoder
â”‚   â”œâ”€â”€ tokenizer.py          # BPE tokenizer wrapper
â”‚   â”œâ”€â”€ utils.py              # RMSNorm, RoPE, helpers
â”‚   â””â”€â”€ training_utils.py     # Training helpers
â”‚
â”œâ”€â”€ configs/                   # JSON configurations
â”‚   â”œâ”€â”€ thinker_tiny.json     # Thinker config
â”‚   â”œâ”€â”€ audio_enc_tiny.json   # Audio encoder config
â”‚   â”œâ”€â”€ vision_tiny.json      # Vision encoder config
â”‚   â”œâ”€â”€ talker_tiny.json      # Talker config
â”‚   â””â”€â”€ omni_sft_tiny.json    # Multimodal SFT config
â”‚
â”œâ”€â”€ scripts/                   # Utility scripts
â”‚   â”œâ”€â”€ check_setup.py        # Verify installation
â”‚   â”œâ”€â”€ download_production_text.py  # Download text data
â”‚   â”œâ”€â”€ download_production_audio.py # Download audio data
â”‚   â”œâ”€â”€ download_production_image.py # Download image data
â”‚   â”œâ”€â”€ download_production_ocr.py   # Download OCR data
â”‚   â”œâ”€â”€ update_configs_from_data.py # Auto-update configs from data
â”‚   â””â”€â”€ make_synthetic_datasets.py   # Generate test data
â”‚
â”œâ”€â”€ train_text.py             # Stage A: Thinker pretraining
â”œâ”€â”€ train_audio_enc.py        # Stage B: Audio encoder
â”œâ”€â”€ train_vision.py           # Stage C: Vision encoder
â”œâ”€â”€ train_talker.py           # Stage D: Talker + RVQ
â”œâ”€â”€ train_vocoder.py          # Optional: HiFi-GAN vocoder
â”œâ”€â”€ train_ocr.py              # Optional: OCR model
â”œâ”€â”€ sft_omni.py              # Stage E: Multimodal SFT
â”‚
â”œâ”€â”€ infer_chat.py            # Inference interface
â”œâ”€â”€ test_all_media.py        # Test multimodal inputs
â”‚
â”œâ”€â”€ data/                    # Training data (create)
â”‚   â”œâ”€â”€ text/                # Text corpus files
â”‚   â”‚   â””â”€â”€ *.line_offsets.pkl  # Cached offset indices (auto-generated)
â”‚   â”œâ”€â”€ images/              # Image manifest files
â”‚   â”‚   â””â”€â”€ *.json_offsets.pkl  # Cached offset indices (auto-generated)
â”‚   â”œâ”€â”€ audio/               # Audio CSV files
â”‚   â”‚   â””â”€â”€ *.row_offsets.pkl   # Cached offset indices (auto-generated)
â”‚   â””â”€â”€ ocr/                 # OCR CSV files
â”‚       â””â”€â”€ *.row_offsets.pkl   # Cached offset indices (auto-generated)
â”‚
â”œâ”€â”€ checkpoints/             # Model weights (create)
â”‚   â”œâ”€â”€ thinker_tiny/
â”‚   â”œâ”€â”€ audio_enc_tiny/
â”‚   â”œâ”€â”€ vision_tiny/
â”‚   â”œâ”€â”€ talker_tiny/
â”‚   â””â”€â”€ omni_sft_tiny/
â”‚
â”œâ”€â”€ examples/                # Sample inputs
â”‚   â”œâ”€â”€ sample_image.png
â”‚   â”œâ”€â”€ sample_audio.wav
â”‚   â””â”€â”€ sample_text.txt
â”‚
â”œâ”€â”€ study/                   # Documentation (this!)
â”‚   â”œâ”€â”€ 00-INDEX.md
â”‚   â”œâ”€â”€ 01-what-is-ai.md
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ requirements.txt         # Python dependencies
â””â”€â”€ README.md               # Main README
```

---

## ğŸ” Key Files Explained

### Core Modules (`omni/`)

#### `thinker.py`
```python
class ThinkerLM(nn.Module):
    """
    Decoder-only transformer (GPT-style)
    - Accepts token IDs or embeddings
    - Causal attention with RoPE
    - KV caching for fast generation
    """
```

#### `audio_encoder.py`
```python
class AudioEncoderTiny(nn.Module):
    """
    Audio understanding encoder
    - Input: Mel spectrogram (T, 128)
    - Process: Conv downsample + Transformer
    - Output: Frame embeddings (T/8, 192)
    """
```

#### `vision_encoder.py`
```python
class ViTTiny(nn.Module):
    """
    Vision Transformer encoder
    - Input: Image (224Ã—224Ã—3)
    - Process: Patch embedding + Transformer
    - Output: CLS token (1, 128)
    """
```

#### `talker.py`
```python
class TalkerTiny(nn.Module):
    """
    Speech code generator
    - Input: Previous RVQ codes
    - Process: Transformer decoder
    - Output: Next frame codes (base + residual)
    """
```

#### `codec.py`
```python
class RVQ(nn.Module):
    """Residual Vector Quantization"""

class GriffinLimVocoder:
    """Classical vocoder (no training)"""
```

---

### Training Scripts

#### `train_text.py`
- **Stage A**: Thinker pretraining
- **Data**: Text corpus
- **Loss**: Cross-entropy (next-token)
- **Output**: `checkpoints/thinker_tiny/`

#### `train_audio_enc.py`
- **Stage B**: Audio encoder ASR
- **Data**: Audio + transcriptions
- **Loss**: CTC
- **Output**: `checkpoints/audio_enc_tiny/`

#### `train_vision.py`
- **Stage C**: Vision encoder
- **Data**: Images + captions
- **Loss**: Cross-entropy
- **Output**: `checkpoints/vision_tiny/`

#### `train_talker.py`
- **Stage D**: Talker + RVQ
- **Data**: Audio files
- **Loss**: Cross-entropy + MSE
- **Output**: `checkpoints/talker_tiny/`

#### `train_vocoder.py`
- **Optional**: HiFi-GAN vocoder training
- **Data**: Audio files (TTS/ASR CSV)
- **Loss**: Adversarial
- **Output**: `checkpoints/vocoder_tiny/`

#### `train_ocr.py`
- **Optional**: OCR model training
- **Data**: Images + text labels
- **Loss**: Cross-entropy
- **Output**: `checkpoints/ocr_tiny/`

#### `sft_omni.py`
- **Stage E**: Multimodal SFT
- **Data**: Mixed modalities (text, images, audio)
- **Loss**: Cross-entropy
- **Output**: `checkpoints/omni_sft_tiny/`

---

### Configuration Files

All configs are JSON:

```json
{
  "model_params": { ... },
  "training_params": { ... },
  "data_params": { ... }
}
```

See [Chapter 34: Configuration Files](34-configuration-files.md) for details.

---

## ğŸ’¡ Code Navigation Tips

### Find Component Definition
```bash
# Thinker architecture
grep -n "class ThinkerLM" omni/thinker.py

# Attention implementation
grep -n "class Attention" omni/thinker.py

# RVQ codec
grep -n "class RVQ" omni/codec.py
```

### Find Training Loop
```bash
# Thinker training
grep -n "def train" train_text.py

# SFT training
grep -n "def train" sft_omni.py
```

### Find Inference Code
```bash
# Generation function
grep -n "def generate" infer_chat.py

# Multimodal processing
grep -n "multimodal" infer_chat.py
```

---

## ğŸ“Š File Dependencies

```
thinker.py
â”œâ”€â”€ utils.py (RMSNorm, RoPE)
â””â”€â”€ (no other omni deps)

audio_encoder.py
â”œâ”€â”€ utils.py (RMSNorm)
â””â”€â”€ (no other omni deps)

vision_encoder.py
â”œâ”€â”€ utils.py (RMSNorm)
â””â”€â”€ (no other omni deps)

talker.py
â”œâ”€â”€ utils.py (RMSNorm, RoPE)
â””â”€â”€ (no other omni deps)

codec.py
â””â”€â”€ (standalone)

infer_chat.py
â”œâ”€â”€ thinker.py
â”œâ”€â”€ audio_encoder.py
â”œâ”€â”€ vision_encoder.py
â”œâ”€â”€ talker.py
â”œâ”€â”€ codec.py
â””â”€â”€ tokenizer.py
```

---

## ğŸ’¾ Cache Files (Auto-Generated)

Training scripts automatically create cache files to speed up dataset initialization:

- **Text files**: `{file_path}.line_offsets.pkl` - Cached line offset indices
- **CSV files**: `{file_path}.row_offsets.pkl` - Cached row offset indices + fieldnames
- **JSON files**: `{file_path}.json_offsets.pkl` - Cached JSON object offsets

These cache files:
- âœ… Created automatically on first run
- âœ… Validated using file modification times
- âœ… Rebuilt automatically if source file changes
- âœ… Can be safely deleted (will regenerate)
- âœ… Significantly speed up dataset initialization

See [Chapter 36: Optimization Techniques](36-optimization-techniques.md) for details.

---

## ğŸ’¡ Key Takeaways

âœ… **Modular structure** - each component independent  
âœ… **Clear separation** - training vs inference  
âœ… **Config-driven** - easy to modify parameters  
âœ… **Self-contained** - minimal dependencies  
âœ… **Offset index caching** - fast dataset initialization

---

[Continue to Chapter 34: Configuration Files â†’](34-configuration-files.md)

