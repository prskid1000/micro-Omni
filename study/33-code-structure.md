# Chapter 33: Codebase Structure Guide

[â† Previous: Inference Pipeline](32-inference-pipeline.md) | [Back to Index](00-INDEX.md) | [Next: Configuration Files â†’](34-configuration-files.md)

---

## ğŸ“‚ Directory Structure

```
Î¼Omni/
â”œâ”€â”€ omni/                      # Core modules
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ thinker.py            # Decoder-only LLM (20.32M params)
â”‚   â”œâ”€â”€ audio_encoder.py      # AuT-Tiny (2.05M params)
â”‚   â”œâ”€â”€ vision_encoder.py     # ViT-Tiny (914K params)
â”‚   â”œâ”€â”€ talker.py             # Speech generator (2.24M params)
â”‚   â”œâ”€â”€ codec.py              # RVQ + Griffin-Lim vocoder + HiFi-GAN neural vocoder
â”‚   â”œâ”€â”€ tokenizer.py          # BPE tokenizer wrapper
â”‚   â””â”€â”€ utils.py              # All utilities (RMSNorm, RoPE, training helpers, datasets, checkpoint loading)
â”‚
â”œâ”€â”€ configs/                   # JSON configurations
â”‚   â”œâ”€â”€ thinker_tiny.json     # Thinker config
â”‚   â”œâ”€â”€ audio_enc_tiny.json   # Audio encoder config
â”‚   â”œâ”€â”€ vision_tiny.json      # Vision encoder config
â”‚   â”œâ”€â”€ talker_tiny.json      # Talker config
â”‚   â””â”€â”€ omni_sft_tiny.json    # Multimodal SFT config
â”‚
â”œâ”€â”€ scripts/                   # Utility scripts
â”‚   â”œâ”€â”€ check_setup.py        # Verify installation
â”‚   â”œâ”€â”€ download_production_text.py  # Download text data
â”‚   â”œâ”€â”€ download_production_audio.py # Download audio data
â”‚   â”œâ”€â”€ download_production_image.py # Download image data
â”‚   â”œâ”€â”€ download_production_ocr.py   # Download OCR data
â”‚   â”œâ”€â”€ update_configs_from_data.py # Auto-update configs from data
â”‚   â””â”€â”€ make_synthetic_datasets.py   # Generate test data
â”‚
â”œâ”€â”€ train_text.py             # Stage A: Thinker pretraining
â”œâ”€â”€ train_audio_enc.py        # Stage B: Audio encoder
â”œâ”€â”€ train_vision.py           # Stage C: Vision encoder
â”œâ”€â”€ train_talker.py           # Stage D: Talker + RVQ
â”œâ”€â”€ train_vocoder.py          # Optional: HiFi-GAN vocoder
â”œâ”€â”€ train_ocr.py              # Optional: OCR model
â”œâ”€â”€ sft_omni.py              # Stage E: Multimodal SFT
â”‚
â”œâ”€â”€ infer_chat.py            # Inference interface
â”œâ”€â”€ test_all_media.py        # Test multimodal inputs
â”‚
â”œâ”€â”€ data/                    # Training data (create)
â”‚   â”œâ”€â”€ text/                # Text corpus files
â”‚   â”œâ”€â”€ images/              # Image manifest files
â”‚   â”œâ”€â”€ audio/               # Audio CSV files
â”‚   â””â”€â”€ ocr/                 # OCR CSV files
â”‚
â”œâ”€â”€ checkpoints/             # Model weights (create)
â”‚   â”œâ”€â”€ thinker_tiny/
â”‚   â”œâ”€â”€ audio_enc_tiny/
â”‚   â”œâ”€â”€ vision_tiny/
â”‚   â”œâ”€â”€ talker_tiny/
â”‚   â””â”€â”€ omni_sft_tiny/
â”‚
â”œâ”€â”€ examples/                # Sample inputs
â”‚   â”œâ”€â”€ sample_image.png
â”‚   â”œâ”€â”€ sample_audio.wav
â”‚   â””â”€â”€ sample_text.txt
â”‚
â”œâ”€â”€ study/                   # Documentation (this!)
â”‚   â”œâ”€â”€ 00-INDEX.md
â”‚   â”œâ”€â”€ 01-what-is-ai.md
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ requirements.txt         # Python dependencies
â””â”€â”€ README.md               # Main README
```

---

## ğŸ” Key Files Explained

### Core Modules (`omni/`)

#### `thinker.py`

```python
class ThinkerLM(nn.Module):
    """
    Decoder-only transformer (GPT-style)
    - Accepts token IDs or embeddings
    - Causal attention with RoPE
    - KV caching for fast generation
    """
```

#### `audio_encoder.py`

```python
class AudioEncoderTiny(nn.Module):
    """
    Audio understanding encoder
    - Input: Mel spectrogram (T, 128)
    - Process: Conv downsample + Transformer
    - Output: Frame embeddings (T/8, 192)
    """
```

#### `vision_encoder.py`

```python
class ViTTiny(nn.Module):
    """
    Vision Transformer encoder
    - Input: Image (224Ã—224Ã—3)
    - Process: Patch embedding + Transformer
    - Output: CLS token (1, 128)
    """
```

#### `talker.py`

```python
class TalkerTiny(nn.Module):
    """
    Speech code generator
    - Input: Previous RVQ codes
    - Process: Transformer decoder
    - Output: Next frame codes (base + residual)
    """
```

#### `codec.py`

```python
class RVQ(nn.Module):
    """Residual Vector Quantization"""

class GriffinLimVocoder:
    """Classical vocoder (no training)"""
```

---

### Training Scripts

#### `train_text.py`

- **Stage A**: Thinker pretraining
- **Data**: Text corpus
- **Loss**: Cross-entropy (next-token)
- **Output**: `checkpoints/thinker_tiny/`

#### `train_audio_enc.py`

- **Stage B**: Audio encoder ASR
- **Data**: Audio + transcriptions
- **Loss**: CTC
- **Output**: `checkpoints/audio_enc_tiny/`

#### `train_vision.py`

- **Stage C**: Vision encoder
- **Data**: Images + captions
- **Loss**: Contrastive (InfoNCE) - vision-language alignment
- **Text Encoding**: Configurable - Thinker model (frozen) or simple tokenizer+embedding
- **Output**: `checkpoints/vision_tiny/`

#### `train_talker.py`

- **Stage D**: Talker + RVQ
- **Data**: Audio files
- **Loss**: Cross-entropy + MSE
- **Output**: `checkpoints/talker_tiny/`

#### `train_vocoder.py`

- **Optional**: HiFi-GAN vocoder training
- **Data**: Audio files (TTS/ASR CSV)
- **Loss**: Adversarial (LSGAN) + Feature Matching + Mel Loss
- **Output**: `checkpoints/vocoder_tiny/`
- **Architecture**: Generator (MRF blocks) + Multi-Period Discriminator + Multi-Scale Discriminator
- **Note**: Generator correctly handles tensor dimensions, audio loading has automatic fallback

#### `train_ocr.py`

- **Optional**: OCR model training
- **Data**: Images + text labels (CSV format)
- **Architecture**: ViT encoder + Transformer decoder with cross-attention
- **Features**: RoPE, SwiGLU, Flash Attention, KV caching
- **Loss**: Cross-entropy (character-level)
- **Output**: `checkpoints/ocr_tiny/`

#### `sft_omni.py`

- **Stage E**: Multimodal SFT
- **Data**: Mixed modalities (text, images, audio)
- **Loss**: Cross-entropy
- **Output**: `checkpoints/omni_sft_tiny/`

---

### Configuration Files

All configs are JSON:

```json
{
  "model_params": { ... },
  "training_params": { ... },
  "data_params": { ... }
}
```

See [Chapter 34: Configuration Files](34-configuration-files.md) for details.

---

## ğŸ’¡ Code Navigation Tips

### Find Component Definition

```bash
# Thinker architecture
grep -n "class ThinkerLM" omni/thinker.py

# Attention implementation
grep -n "class Attention" omni/thinker.py

# RVQ codec
grep -n "class RVQ" omni/codec.py
```

### Find Training Loop

```bash
# Thinker training
grep -n "def train" train_text.py

# SFT training
grep -n "def train" sft_omni.py
```

### Find Inference Code

```bash
# Generation function
grep -n "def generate" infer_chat.py

# Multimodal processing
grep -n "multimodal" infer_chat.py
```

---

## ğŸ“Š File Dependencies

```
thinker.py
â”œâ”€â”€ utils.py (RMSNorm, RoPE)
â””â”€â”€ (no other omni deps)

audio_encoder.py
â”œâ”€â”€ utils.py (RMSNorm)
â””â”€â”€ (no other omni deps)

vision_encoder.py
â”œâ”€â”€ utils.py (RMSNorm)
â””â”€â”€ (no other omni deps)

talker.py
â”œâ”€â”€ utils.py (RMSNorm, RoPE)
â””â”€â”€ (no other omni deps)

codec.py
â””â”€â”€ (standalone)

infer_chat.py
â”œâ”€â”€ thinker.py
â”œâ”€â”€ audio_encoder.py
â”œâ”€â”€ vision_encoder.py
â”œâ”€â”€ talker.py
â”œâ”€â”€ codec.py
â”œâ”€â”€ tokenizer.py
â””â”€â”€ utils.py (find_checkpoint)

train_*.py, sft_omni.py
â”œâ”€â”€ utils.py (training utilities, datasets, checkpoint management)
â””â”€â”€ (model modules)
```

---

## ğŸ’¾ Streaming Datasets

All training scripts use streaming `IterableDataset` implementations:

- **Text files**: Stream line-by-line directly
- **CSV files**: Use `csv.DictReader` for row-by-row streaming
- **JSON files**: Load once, then iterate through items

**Benefits:**

- âœ… No cache files needed - simpler and cleaner
- âœ… Minimal memory usage - only current item in memory
- âœ… Efficient resuming via `skip_samples` parameter
- âœ… Worker sharding for multi-process data loading
- âœ… Buffer-based shuffling for randomization

See [Chapter 36: Optimization Techniques](36-optimization-techniques.md) for details.

## ğŸ”„ Common Training Utilities

All training scripts share common utilities from `omni/utils.py`:

### Collate Functions

All collate functions are centralized in `utils.py` for reuse:

- **`collate_mel_fn(batch, max_mel_length=None)`** - Used by `train_talker.py`
  - Pads mel spectrograms to fixed length for CUDA graphs compatibility
- **`collate_mel_text_fn(batch, max_mel_length=None)`** - Used by `train_audio_enc.py`
  - Pads mel spectrograms and returns text list for ASR training
- **`collate_mel_audio_fn(batch, max_mel_length=None, max_audio_length=None)`** - Used by `train_vocoder.py`
  - Pads both mel spectrograms and audio waveforms for vocoder training

**Benefits:**

- âœ… Consistent padding logic across all training scripts
- âœ… Supports fixed-length padding for CUDA graphs
- âœ… Easy to maintain and update

### Gradient Handling

All training scripts use consistent gradient handling:

- **Clip first, then check** - Gradients are clipped to `max_grad_norm` before checking for explosion
- **Robust threshold** - Only skips batches if gradients exceed 100.0 after clipping
- **Automatic recovery** - Most gradient issues are resolved by clipping, allowing training to continue

### Checkpoint Management

- **`load_checkpoint()`**: Automatically finds and loads the latest checkpoint

  - Prioritizes `model.pt` + `model_metadata.json` (new system)
  - Falls back to legacy step checkpoints (`*_step_*.pt`)
  - Handles model, optimizer, scheduler, and scaler state dicts
  - Returns step number and metadata (including config)

- **`find_checkpoint()`**: Smart checkpoint finder for inference/export
  - First tries standard checkpoint (e.g., `thinker.pt`)
  - If not found, automatically searches for latest step checkpoint (e.g., `thinker_step_*.pt`)
  - Returns the checkpoint path and loaded data
  - Used by `infer_chat.py` and `export.py` to handle interrupted training gracefully

### Resuming Training

- **`setup_resume_data_loading()`**: Configures dataset `skip_samples` for resuming

  - Handles `SubsetDataset` wrappers from `random_split`
  - Recreates DataLoader with updated skip_samples
  - Works seamlessly with IterableDataset streaming

- **`calculate_resume_position()`**: Calculates epoch and batch position from global step

  - Returns `(start_epoch, start_batch_idx)` tuple
  - Used for progress bar initialization and epoch tracking

- **Automatic skip_samples reset**: All `IterableDataset` classes automatically reset `skip_samples` to 0 after each iteration completes
  - Implemented in the `__iter__` method of each dataset class
  - Ensures subsequent epochs always start from the beginning
  - Works correctly even if dataset is exhausted mid-epoch

### Validation

- **`ValidationSkipSamplesContext`**: Context manager for validation loops
  - Temporarily resets `skip_samples` to 0 for validation
  - Ensures validation always processes full validation set
  - Automatically restores original `skip_samples` after validation

**Benefits:**

- âœ… Consistent resuming logic across all training scripts
- âœ… Automatic checkpoint detection (no `--resume` flag needed)
- âœ… Proper validation on full dataset regardless of training resumption
- âœ… Automatic dataset reset for multi-epoch training
- âœ… Graceful handling of datasets smaller than one epoch or total epochs
- âœ… Reduced code duplication and easier maintenance

---

## ğŸ’¡ Key Takeaways

âœ… **Modular structure** - each component independent  
âœ… **Clear separation** - training vs inference  
âœ… **Config-driven** - easy to modify parameters  
âœ… **Self-contained** - minimal dependencies  
âœ… **Streaming datasets** - efficient memory usage

---

[Continue to Chapter 34: Configuration Files â†’](34-configuration-files.md)
