# Chapter 17: Mixture of Experts (MoE)

[â† Previous: SwiGLU](16-swiglu-activation.md) | [Back to Index](00-INDEX.md) | [Next: Normalization â†’](18-normalization.md)

---

## ğŸ¯ Core Idea

Instead of one large feedforward network, use **multiple expert networks** and route each token to a subset.

## ğŸ—ï¸ Architecture

```
Input token
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Router Network     â”‚ â†’ Selects top-k experts
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
Probabilities: [0.1, 0.05, 0.45, 0.02, 0.38, ...]
Top-2 experts: Expert 2 (0.45), Expert 4 (0.38)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Expert 2  â”‚   â”‚ Expert 4  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“               â†“
output_2 Ã— 0.45 + output_4 Ã— 0.38
    â†“
Combined output
```

## âœ… Benefits

```
Traditional FFN:
All tokens â†’ Same large network
Cost: O(tokens Ã— FFN_size)

MoE:
Each token â†’ Top-k of N experts
Cost: O(tokens Ã— (FFN_size / N) Ã— k)

If N=8, k=2: Cost = 1/4 of traditional!
But total capacity = Same or more
```

## ğŸ“Š Trade-offs

| Feature | Dense FFN | MoE |
|---------|-----------|-----|
| **Computation** | High | Low (sparse) |
| **Parameters** | Lower | Higher |
| **Capacity** | Limited | High |
| **Complexity** | Simple | Complex (routing) |

## ğŸ’¡ Key Takeaways

âœ… **MoE** = Multiple expert networks + router  
âœ… **Sparse activation** (only k of N experts used)  
âœ… **Higher capacity** with same computation  
âœ… **Î¼Omni supports MoE** (optional, experimental)

---

[Back to Index](00-INDEX.md)

