# Chapter 48: OCR Model - Text Extraction from Images

[â† Previous: Model Export & Deployment](46-model-export-deployment.md) | [Back to Index](00-INDEX.md) | [Next: Future Extensions â†’](45-future-extensions.md)

---

## ğŸ¯ Learning Objectives

By the end of this chapter, you will understand:

- What OCR is and why we need it
- Architecture: ViT encoder + Transformer decoder with cross-attention
- How the model extracts text from images
- RoPE positional encoding for text sequences
- Cross-attention mechanism connecting vision and text
- Training process and data requirements
- Integration with multimodal understanding

---

## ğŸ’¡ What is OCR?

### Optical Character Recognition

**Analogy: Reading a Book**

```
Think of OCR like reading text from an image:

IMAGE WITH TEXT:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Photo of a sign: "STOP"
â†“
Human can read: "S-T-O-P"
â†“
Computer needs to: Extract "STOP" as text

OCR MODEL:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Input: Image (224Ã—224Ã—3)
â†“
Vision Encoder: Understands image content
â†“
Text Decoder: Generates characters autoregressively
â†“
Output: "STOP" (text string)

The OCR model is the TEXT READER:
Images â†’ Text extraction!
```

**Why Do We Need This?**

```
Problem: Images contain text, but models see pixels!
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Multimodal understanding needs:
âŒ Vision encoder sees: "orange pixels, white pixels"
âŒ Doesn't know: "This says 'STOP'"
âŒ Can't extract: Text content from images

Solution: OCR Model!
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

OCR enables:
âœ… Extract text from images
âœ… Understand document content
âœ… Process screenshots, signs, labels
âœ… Enhance multimodal reasoning
âœ… Combine visual + textual understanding

Use cases:
- Document processing
- Screenshot analysis
- Sign reading
- Handwritten text recognition
- Multimodal question answering
```

---

## ğŸ—ï¸ Architecture Overview

### Two-Component System

```
OCR MODEL ARCHITECTURE:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Image (B, 3, 224, 224)
    â†“
[Vision Encoder (ViT)]
    â†“
Image Features (B, N, 128)
    â†“
[Image Projection]
    â†“
Image Features (B, N, 256)
    â†“
[Text Decoder]
    â”œâ”€ Self-Attention (causal, with RoPE)
    â”œâ”€ Cross-Attention (to image features)
    â””â”€ Feedforward (SwiGLU)
    â†“
Character Logits (B, T, vocab_size)
    â†“
Text: "STOP"
```

### Component Breakdown

**1. Vision Encoder (ViT-Tiny)**

- Input: Image `(B, 3, 224, 224)`
- Process: Patch embedding + Transformer layers
- Output: Grid features `(B, N, 128)` where N = (224/16)Â² = 196 patches
- Purpose: Extract visual features from image patches

**2. Image Projection**

- Projects vision features from 128-dim to decoder dimension (256-dim)
- Aligns vision and text embeddings

**3. Text Decoder**

- Input: Character token IDs `(B, T)`
- Process: Autoregressive generation with cross-attention
- Output: Character logits `(B, T, vocab_size)`
- Purpose: Generate text from visual features

---

## ğŸ”§ Decoder Architecture

### OCRDecoderBlock Structure

```
DECODER BLOCK (per layer):
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Input: Text embeddings (B, T, 256)
    â†“
[1. Self-Attention (Causal)]
    â”œâ”€ Norm1 (RMSNorm)
    â”œâ”€ Attention (with RoPE)
    â””â”€ Residual + Dropout
    â†“
[2. Cross-Attention (to Image)]
    â”œâ”€ Norm2 (RMSNorm)
    â”œâ”€ MultiheadAttention (query=text, key/value=image)
    â””â”€ Residual + Dropout
    â†“
[3. Feedforward]
    â”œâ”€ Norm3 (RMSNorm)
    â”œâ”€ MLP (SwiGLU)
    â””â”€ Residual + Dropout
    â†“
Output: (B, T, 256)
```

### Key Features

**1. Separate Norm Instances**

- Each sub-layer has its own `RMSNorm` instance
- Prevents parameter sharing across layers
- Matches Thinker's Block pattern

**2. RoPE for Self-Attention**

- Rotary Position Embedding applied to queries and keys
- Enables relative position understanding
- Supports longer sequences than training

**3. Cross-Attention Mechanism**

- Text tokens (query) attend to image features (key/value)
- Allows decoder to "look" at relevant image regions
- No RoPE needed (cross-attention doesn't use positional encoding)

**4. Causal Masking**

- Self-attention is causal (can only see previous tokens)
- Enables autoregressive text generation
- Prevents information leakage

---

## ğŸ“ Mathematical Details

### Forward Pass Flow

```
1. IMAGE ENCODING:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Image: (B, 3, 224, 224)
    â†“ ViT Encoder
Grid: (B, 196, 128)  [196 = (224/16)Â² patches]
    â†“ Image Projection
Img Features: (B, 196, 256)

2. TEXT EMBEDDING:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Text IDs: (B, T)  [T = sequence length]
    â†“ Character Embedding
Text Embed: (B, T, 256)

3. DECODER PROCESSING:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
For each decoder layer:
    x = Text Embed (B, T, 256)

    # Self-attention (causal)
    x = x + Dropout(SelfAttn(Norm1(x), RoPE))

    # Cross-attention
    x = x + Dropout(CrossAttn(Norm2(x), Img Features))

    # Feedforward
    x = x + Dropout(MLP(Norm3(x)))

4. OUTPUT:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
x = Norm(x)  # Final normalization
Logits = Linear(x)  # (B, T, vocab_size)
```

### Attention Mechanisms

**Self-Attention (Causal)**

```
Query: Q = Text Embeddings (B, T, 256)
Key:   K = Text Embeddings (B, T, 256)
Value: V = Text Embeddings (B, T, 256)

Apply RoPE to Q and K
Apply causal mask (lower triangular)
Attention = Softmax(QK^T / âˆšd) V
```

**Cross-Attention**

```
Query: Q = Text Embeddings (B, T, 256)
Key:   K = Image Features (B, N, 256)
Value: V = Image Features (B, N, 256)

No RoPE (cross-attention doesn't need position)
No causal mask (can attend to all image patches)
Attention = Softmax(QK^T / âˆšd) V
```

---

## ğŸ“ Training Process

### Data Format

```
CSV Format:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
image,text
data/ocr/img1.jpg,"STOP"
data/ocr/img2.jpg,"Hello World"
data/ocr/img3.jpg,"123 Main St"

Requirements:
- Images: Any format (JPG, PNG, etc.)
- Text: Plain text strings
- Character vocabulary: Built from dataset
```

### Training Objective

```
TEACHER FORCING:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Input:  [BOS] + "S" + "T" + "O"
Target: "S" + "T" + "O" + [EOS]

Model predicts next character given:
- Previous characters (self-attention)
- Image features (cross-attention)

Loss: Cross-entropy on character predictions

**Expected Validation Loss:**
- Target Loss: < 1.0
- Target Character Accuracy: > 90%
- Good: loss < 0.8, accuracy > 95%
- Excellent: loss < 0.5, accuracy > 98%
Ignore: PAD tokens (index 0)
```

### Training Configuration

```json
{
  "img_size": 224,
  "patch": 16,
  "vision_d_model": 128,
  "vision_layers": 4,
  "vision_heads": 2,
  "vision_d_ff": 512,
  "decoder_d_model": 256,
  "decoder_layers": 4,
  "decoder_heads": 4,
  "decoder_d_ff": 1024,
  "dropout": 0.1,
  "use_gqa": false,
  "use_swiglu": true,
  "use_flash": true,
  "rope_theta": 10000.0,
  "vocab_size": <dynamic from dataset>
}
```

---

## ğŸš€ Key Features

### 1. Modern Architecture

```
âœ… ViT Encoder: State-of-the-art vision processing
âœ… Transformer Decoder: Autoregressive text generation
âœ… Cross-Attention: Connects vision and text
âœ… RoPE: Relative position encoding
âœ… SwiGLU: Modern activation function
âœ… Flash Attention: 2-4x speedup (optional)
```

### 2. Optimizations

```
âœ… Separate Norm Instances: Per-layer normalization
âœ… KV Caching: Fast autoregressive generation
âœ… Gradient Accumulation: Train with larger effective batch size
âœ… Mixed Precision: FP16 training for efficiency
âœ… Gradient Clipping: Prevents exploding gradients
```

### 3. Integration

```
âœ… Character Vocabulary: Built dynamically from dataset
âœ… Variable Length: Handles different text lengths
âœ… Multimodal Ready: Can be integrated with Thinker
âœ… Inference Support: KV caching for fast generation
```

---

## ğŸ’» Usage Example

### Training

```python
# Train OCR model
python train_ocr.py --config configs/ocr_tiny.json
```

### Inference

```python
from omni.ocr_model import OCRModel
import torch
from PIL import Image
import torchvision.transforms as T

# Load model
model = OCRModel(...)
model.load_state_dict(torch.load("checkpoints/ocr_tiny/model.pt")["model"])
model.eval()

# Process image
image = Image.open("sign.jpg")
transform = T.Compose([
    T.Resize((224, 224)),
    T.ToTensor(),
    T.Normalize(mean=[0.485, 0.456, 0.406],
                std=[0.229, 0.224, 0.225])
])
img_tensor = transform(image).unsqueeze(0)  # (1, 3, 224, 224)

# Generate text
text_ids = torch.tensor([[1]])  # BOS token
model.decoder.enable_kv_cache(True)

for _ in range(max_length):
    logits = model(img_tensor, text_ids)
    next_id = logits[0, -1].argmax().item()
    if next_id == 2:  # EOS
        break
    text_ids = torch.cat([text_ids, torch.tensor([[next_id]])], dim=1)

# Decode text
text = decode_text(text_ids)
print(f"Extracted text: {text}")
```

---

## ğŸ”¬ Architecture Comparison

### Similar to Modern OCR Models

**VISTA-OCR (2024)**

- âœ… ViT encoder + Transformer decoder
- âœ… Cross-attention from text to image
- âœ… Autoregressive text generation

**UPOCR (2023)**

- âœ… Vision Transformer encoder
- âœ… Transformer decoder with cross-attention
- âœ… Unified image-to-text approach

**Our Implementation**

- âœ… Matches modern OCR architectures
- âœ… Uses proven components (ViT, Transformer)
- âœ… Optimized with Flash Attention, RoPE, SwiGLU

---

## ğŸ“Š Model Parameters

### Tiny Configuration

```
Vision Encoder (ViT-Tiny):
- Layers: 4
- Heads: 2
- Dimension: 128
- FFN: 512
- Patches: 196 (14Ã—14)

Text Decoder:
- Layers: 4
- Heads: 4
- Dimension: 256
- FFN: 1024
- Vocabulary: Dynamic (from dataset)

Total Parameters: ~15-20M (depends on vocab size)
```

---

## ğŸ¯ Key Takeaways

âœ… **OCR extracts text from images** using vision encoder + text decoder  
âœ… **Cross-attention** connects visual features to text generation  
âœ… **RoPE** enables relative position understanding in text sequences  
âœ… **Separate norms** per layer (matches Thinker pattern)  
âœ… **Autoregressive generation** with causal masking  
âœ… **KV caching** for fast inference  
âœ… **Character-level** vocabulary built from dataset  
âœ… **Modern architecture** aligned with state-of-the-art OCR models

---

## ğŸ”— Related Chapters

- [Chapter 22: Vision Encoder](22-vision-encoder.md) - ViT architecture
- [Chapter 13: Decoder-Only LLM](13-decoder-only-llm.md) - Transformer decoder
- [Chapter 08: Positional Encoding](08-positional-encoding.md) - RoPE details
- [Chapter 07: Attention Mechanism](07-attention-mechanism.md) - Attention basics
- [Chapter 16: SwiGLU Activation](16-swiglu-activation.md) - SwiGLU details
- [Chapter 15: GQA Attention](15-gqa-attention.md) - Grouped Query Attention

---

[â† Previous: Model Export & Deployment](46-model-export-deployment.md) | [Back to Index](00-INDEX.md) | [Next: Future Extensions â†’](45-future-extensions.md)
