# Chapter 24: The Talker - Speech Generator

[Back to Index](00-INDEX.md)

---

## ğŸ¯ Purpose

Generate speech by autoregressively predicting RVQ codes.

## ğŸ—ï¸ Architecture

```
Previous Codes (B, T, 2)
    â†“
Embed Base + Residual Codes
    â†“
Transformer Decoder (4 layers)
  - Causal attention
  - RoPE
  - KV caching
    â†“
Separate Heads:
  - Base Head â†’ (B, T, 128) logits
  - Residual Head â†’ (B, T, 128) logits
    â†“
Predict: [base_code, res_code]
```

## ğŸ“Š Specifications

| Parameter | Value |
|-----------|-------|
| **Dimension** | 192 |
| **Layers** | 4 |
| **Heads** | 3 |
| **Codebooks** | 2 |
| **Output** | 2 Ã— 128 logits |
| **Parameters** | ~10-15M |

## ğŸ”„ Generation Process

```
1. Start: codes = [[0, 0]]  (start token)

2. Predict next frame:
   base_logits, res_logits = talker(codes)
   base = argmax(base_logits)  # â†’ 42
   res = argmax(res_logits)    # â†’ 87
   codes = [[0,0], [42,87]]

3. Repeat for T frames...

4. Decode with RVQ:
   mel = rvq.decode(codes)

5. Vocode with Griffin-Lim:
   audio = vocoder.mel_to_audio(mel)
```

## ğŸ’¡ Key Takeaways

âœ… **Autoregressive** code prediction  
âœ… **2 separate heads** (base + residual)  
âœ… **Uses KV caching** for speed  
âœ… **Works with RVQ + Griffin-Lim** vocoder

---

[Back to Index](00-INDEX.md)

