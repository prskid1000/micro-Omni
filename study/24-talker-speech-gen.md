# Chapter 24: The Talker - Speech Generator

[â† Previous: RVQ Codec](23-codec-rvq.md) | [Back to Index](00-INDEX.md) | [Next: Multimodal Fusion â†’](25-multimodal-fusion.md)

---

## ğŸ¯ Learning Objectives

By the end of this chapter, you will understand:
- What the Talker does and why we need it
- How autoregressive speech code prediction works
- Architecture of the Talker transformer
- The two-head prediction system (base + residual)
- Complete generation process from start to audio
- Training strategy and objectives
- Connection to RVQ and vocoder

---

## ğŸ’¡ What is the Talker?

### The Speech Code Generator

**Analogy: Story Writer**

```
Think of speech generation like writing a story:

TEXT GENERATION (familiar):
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Writer (LLM): "Once upon a ___"
â†“
Predict next word: "time"
â†“
Continue: "Once upon a time there ___"
â†“
Predict next word: "was"
â†“
Story builds word by word!

Each step:
- Look at previous words
- Predict next word
- Append and repeat

SPEECH CODE GENERATION (same idea!):
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Talker: [[0,0]] (start)
â†“
Predict next codes: [42, 87]
â†“
Continue: [[0,0], [42,87]] 
â†“
Predict next codes: [56, 91]
â†“
Speech builds code-pair by code-pair!

Each step:
- Look at previous code pairs
- Predict next [base, residual] codes
- Append and repeat

The Talker is the SPEECH WRITER:
Generates speech codes autoregressively, just like text!
```

**Why Do We Need This?**

```
Problem: How to generate speech?
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Can't generate mel spectrograms directly:
âŒ Continuous values (can't use softmax)
âŒ High dimensional (128 mel bins per frame)
âŒ No clear autoregressive structure

Thanks to RVQ Codec (Chapter 23):
âœ… Mel â†’ Discrete codes [base, residual]
âœ… Finite vocabulary (128 options each)
âœ… Can use softmax like text!

Now we need a model to PREDICT these codes!

Solution: The Talker!
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Talker is a transformer that:
âœ… Looks at previous speech codes
âœ… Predicts next [base, residual] codes
âœ… Uses same mechanism as text generation
âœ… Enables autoregressive speech synthesis!

Complete pipeline:
Talker â†’ Codes â†’ RVQ â†’ Mel â†’ Vocoder â†’ Audio âœ“
```

---

## ğŸ—ï¸ Detailed Architecture Breakdown

### The Complete Talker Pipeline

```
INPUT: Previous speech codes
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Shape: (B, T, 2)
- B = batch size (e.g., 1)
- T = time steps so far (growing!)
- 2 = [base_code, residual_code]

Example at step 3:
codes = [[0, 0],      â† Start token
         [42, 87],    â† Frame 1
         [56, 91]]    â† Frame 2
Shape: (1, 3, 2)

We want to predict Frame 3: [?, ?]

Step 1: Embed the Codes
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Separate embeddings for base and residual!

Base codes: [0, 42, 56]
â†’ base_embedding(0): 192-dim vector
â†’ base_embedding(42): 192-dim vector
â†’ base_embedding(56): 192-dim vector

Residual codes: [0, 87, 91]
â†’ res_embedding(0): 192-dim vector
â†’ res_embedding(87): 192-dim vector
â†’ res_embedding(91): 192-dim vector

Sum embeddings:
token_0 = base_emb[0] + res_emb[0]      # (192,)
token_1 = base_emb[42] + res_emb[87]    # (192,)
token_2 = base_emb[56] + res_emb[91]    # (192,)

Result: (3, 192)

WHY separate embeddings?
- Base and residual codes have different meanings
- Base = coarse pattern
- Residual = fine details
- Separate embeddings capture this distinction!

WHY sum instead of concatenate?
- More parameter efficient
- Both contribute to single token representation
- Standard practice in multi-codebook models

Step 2: Add Positional Embeddings
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

We use RoPE (Rotary Position Embedding) from Chapter 8!
- Applied during attention
- Each position gets unique rotation
- Tokens know their temporal order

Step 3: Transformer Decoder (4 Layers)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Each layer processes the sequence:

Layer 1:
  Input: (3, 192)
  â†’ RMSNorm
  â†’ Causal Self-Attention with RoPE
     - token_0 sees only: [token_0]
     - token_1 sees only: [token_0, token_1]
     - token_2 sees only: [token_0, token_1, token_2]
     (Causal = can't see future!)
  â†’ Feedforward network
  â†’ RMSNorm
  Output: (3, 192)

Layers 2-4: Same structure

After 4 layers:
  Output: (3, 192)
  - Each position has processed context
  - Position 2 (last) aggregated info from 0,1,2
  - Ready to predict next code!

Step 4: Two Separate Prediction Heads
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Take last position output: (192,)

Base Head:
  Linear: 192 â†’ 128 logits
  â†’ Logits for all 128 base codes
  â†’ Softmax â†’ Probabilities
  â†’ Sample or Argmax â†’ base_code = 67

Residual Head:
  Linear: 192 â†’ 128 logits
  â†’ Logits for all 128 residual codes
  â†’ Softmax â†’ Probabilities
  â†’ Sample or Argmax â†’ res_code = 103

WHY separate heads?
- Base and residual are predicted independently
- Each needs own distribution over 128 codes
- Allows model to learn different strategies

OUTPUT
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Predicted next frame: [67, 103]

Append to sequence:
codes = [[0, 0],
         [42, 87],
         [56, 91],
         [67, 103]]  â† NEW!

Ready for next step!
```

### Visual Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  INPUT: Previous Codes                  â”‚
â”‚  [[0,0], [42,87], [56,91]]             â”‚
â”‚  Shape: (3, 2)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  EMBED CODES                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Base Embedding: 128 â†’ 192          â”‚ â”‚
â”‚  â”‚ [0, 42, 56] â†’ (3, 192)             â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Residual Embedding: 128 â†’ 192      â”‚ â”‚
â”‚  â”‚ [0, 87, 91] â†’ (3, 192)             â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚  Sum: base_emb + res_emb               â”‚
â”‚  Output: (3, 192)                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TRANSFORMER DECODER (4 Layers)         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Layer 1: Causal Attention + FFN   â”‚ â”‚
â”‚  â”‚  - RoPE for positions             â”‚ â”‚
â”‚  â”‚  - Can't see future frames!       â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Layer 2: Causal Attention + FFN   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Layer 3: Causal Attention + FFN   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Layer 4: Causal Attention + FFN   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚  Output: (3, 192)                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TAKE LAST POSITION                     â”‚
â”‚  Extract position 2: (192,)             â”‚
â”‚  This predicts the NEXT frame           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†“
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
         â†“                â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  BASE HEAD      â”‚  â”‚ RESIDUAL HEAD   â”‚
â”‚  Linear: 192â†’128â”‚  â”‚ Linear: 192â†’128 â”‚
â”‚  Logits: (128,) â”‚  â”‚ Logits: (128,)  â”‚
â”‚  Softmax        â”‚  â”‚ Softmax         â”‚
â”‚  Sample/Argmax  â”‚  â”‚ Sample/Argmax   â”‚
â”‚  â†’ code: 67     â”‚  â”‚ â†’ code: 103     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  PREDICTED NEXT FRAME   â”‚
        â”‚  [67, 103]              â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”„ Complete Generation Process

### From Start Token to Audio

**Step-by-Step Generation:**

```
GENERATION LOOP:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Goal: Generate 200 frames (~16 seconds at 12.5 Hz)

Step 0: Initialize
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
codes = [[0, 0]]  # BOS token
generated_frames = 0
max_frames = 200

Step 1: Generate frame 1
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Input: [[0, 0]]

Talker forward:
1. Embed: (1, 192)
2. Transform: (1, 192)
3. Base head: logits (128,) â†’ softmax â†’ sample â†’ 42
4. Res head: logits (128,) â†’ softmax â†’ sample â†’ 87

Append: codes = [[0, 0], [42, 87]]
generated_frames = 1

Step 2: Generate frame 2
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Input: [[0, 0], [42, 87]]

Talker forward:
1. Embed: (2, 192)
2. Transform: (2, 192)
3. Take last position: (192,)
4. Base head: â†’ 56
5. Res head: â†’ 91

Append: codes = [[0, 0], [42, 87], [56, 91]]
generated_frames = 2

...continue for 200 frames...

Step 200: Final frame
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
codes shape: (201, 2)  # 1 BOS + 200 generated

Remove BOS: codes = codes[1:]  # (200, 2)

DECODING TO AUDIO:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Step 1: RVQ Decode (Codes â†’ Mel)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
mel_frames = []
for i in range(200):
    code_pair = codes[i]  # [base, residual]
    mel_frame = rvq.decode(code_pair)  # (128,)
    mel_frames.append(mel_frame)

mel_spectrogram = stack(mel_frames)  # (200, 128)

Step 2: Vocoder (Mel â†’ Audio)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# Uses HiFi-GAN if available, falls back to Griffin-Lim
audio_waveform = vocoder.mel_to_audio(mel_spectrogram)

Step 3: Save Audio
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
save_wav("generated_speech.wav", audio_waveform, sr=16000)

DONE! ğŸ‰ Speech generated!
```

**Pseudocode:**

```python
def generate_speech(talker, rvq, vocoder, max_frames=200):
    # Start with BOS token
    codes = [[0, 0]]  # (1, 2)
    
    # Generate frames autoregressively
    for t in range(max_frames):
        # Forward pass
        base_logits, res_logits = talker(codes)  # (T, 128), (T, 128)
        
        # Take last position predictions
        base_logits_last = base_logits[-1]  # (128,)
        res_logits_last = res_logits[-1]    # (128,)
        
        # Sample or greedy
        base_code = torch.argmax(base_logits_last)  # scalar
        res_code = torch.argmax(res_logits_last)    # scalar
        
        # Append to sequence
        next_frame = [base_code.item(), res_code.item()]
        codes.append(next_frame)
    
    # Remove BOS token
    codes = codes[1:]  # (200, 2)
    
    # Decode with RVQ
    mel_frames = []
    for code_pair in codes:
        mel = rvq.decode(code_pair)
        mel_frames.append(mel)
    mel_spectrogram = torch.stack(mel_frames)  # (200, 128)
    
    # Vocode (HiFi-GAN if available, else Griffin-Lim)
    audio = vocoder.mel_to_audio(mel_spectrogram)
    
    return audio
```

---

## ğŸ“Š Detailed Specifications

> **Note**: These are the "tiny" configuration values from `configs/talker_tiny.json`. The code defaults may differ, but config files override them.

### Architecture Parameters

```
TALKER CONFIGURATION:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Model dimension: 192
Number of layers: 4
Attention heads: 3 (GQA: 3 query heads, 1 KV head)
FFN dimension: 768 (4 Ã— 192)
Codebook size: 128 (per codebook)
Number of codebooks: 2

Embeddings:
- base_embedding: Embedding(128, 192)
- res_embedding: Embedding(128, 192)

Transformer:
- 4 decoder layers
- Causal self-attention
- RoPE positional encoding
- RMSNorm
- SwiGLU activation (optional)

Prediction Heads:
- base_head: Linear(192 â†’ 128, bias=False)
- res_head: Linear(192 â†’ 128, bias=False)

PARAMETERS:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Embeddings: 2 Ã— (128 Ã— 192) = 49,152
Transformer layers: ~10M
Prediction heads: 2 Ã— (192 Ã— 128) = 49,152
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total: ~10.1M parameters

GENERATION SPECS:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Frame rate: 12.5 Hz (80ms per frame)
Typical length: 200 frames = 16 seconds
With KV caching: ~50-100ms per frame (real-time capable!)
```

### Comparison Table

| Component | Input | Output | Purpose |
|-----------|-------|--------|---------|
| **Embeddings** | Codes (T, 2) | Vectors (T, 192) | Vectorize discrete codes |
| **Transformer** | (T, 192) | (T, 192) | Process temporal context |
| **Base Head** | (192,) | Logits (128,) | Predict base code |
| **Res Head** | (192,) | Logits (128,) | Predict residual code |

---

## ğŸ“ Training the Talker

### Learning to Predict Speech Codes

**Training Objective:**

```
Goal: Given previous codes, predict next codes accurately

Teacher Forcing Strategy:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

During training, use GROUND TRUTH codes:

Ground truth speech: "hello"
â†’ Extract mel spectrogram
â†’ Encode with RVQ: [[0,0], [42,87], [56,91], [12,34], ...]

Input:  [[0,0], [42,87], [56,91]]
Target: [[42,87], [56,91], [12,34]]

Model predicts next code at each position:
Position 0: Given [0,0], predict [42,87]
Position 1: Given [0,0],[42,87], predict [56,91]
Position 2: Given [0,0],[42,87],[56,91], predict [12,34]

Loss: Cross-entropy for both base and residual predictions
```

**Training Loop:**

```python
for batch in dataloader:
    audio = batch  # (B, samples)
    
    # 1. Convert audio to mel
    mel = audio_to_mel(audio)  # (B, T, 128)
    # Note: All mel spectrograms are padded to max_mel_length
    # for CUDA graphs compatibility (when use_compile: true)
    
    # 2. Encode mel with RVQ (frozen!)
    codes = rvq.encode(mel)  # (B, T, 2)
    
    # 3. Prepare input/target
    input_codes = codes[:, :-1, :]   # All but last
    target_codes = codes[:, 1:, :]   # All but first
    
    # 4. Forward pass
    base_logits, res_logits = talker(input_codes)
    # base_logits: (B, T-1, 128)
    # res_logits: (B, T-1, 128)
    
    # 5. Compute loss
    base_loss = cross_entropy(
        base_logits.view(-1, 128),
        target_codes[:, :, 0].view(-1)
    )
    res_loss = cross_entropy(
        res_logits.view(-1, 128),
        target_codes[:, :, 1].view(-1)
    )
    total_loss = base_loss + res_loss
    
    # 6. Backprop and update
    total_loss.backward()
    optimizer.step()
```

**CUDA Graphs Compatibility:**
- When using `use_compile: true`, all batches must have uniform shapes
- `max_mel_length` is auto-calculated from dataset (95th percentile)
- Can override manually or adjust `max_mel_length_percentile` if needed
- Note: Talker uses different frame rate (12.5 Hz with frame_ms=80)
- For 60 seconds: typically ~750 frames (60 Ã— 12.5)
- All mel spectrograms are padded/truncated to this fixed length
- Prevents "tensor size mismatch" errors with CUDA graphs compilation
- See Chapter 34 (Configuration Files) for details

**Key Training Details:**

```
RVQ Codec: FROZEN
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Why frozen?
- RVQ already trained (Stage D-part1)
- Provides stable code targets
- Talker learns to predict these fixed codes

If not frozen:
- Moving target problem
- Codes change during training
- Talker can't learn effectively

Dataset:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Speech audio files (.wav)
â†’ Convert to mel
â†’ Encode to codes
â†’ Train on code prediction

Evaluation:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Perplexity: Measures prediction confidence
MOS (Mean Opinion Score): Human quality rating
Intelligibility: Can humans understand?
```

---

## ğŸ”— Connection to Complete Pipeline

### The Talker in Î¼Omni Ecosystem

```
TEXT-TO-SPEECH IN Î¼Omni:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

1. User input: "Describe this image"
2. Image uploaded
3. Thinker processes:
   - Image embedding (1 token)
   - Text embedding (3 tokens)
   - Generates response: "This is a cat sitting..."
4. User requests speech output
5. Talker generates:
   - Input: Text from Thinker (optional conditioning)
   - Output: Speech codes [[42,87], [56,91], ...]
6. RVQ decodes:
   - Codes â†’ Mel spectrogram
7. Griffin-Lim vocodes:
   - Mel â†’ Audio waveform
8. Play audio: User hears "This is a cat sitting..."

Talker is the SPEECH SYNTHESIZER! â­

TRAINING PIPELINE (Stage D):
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Stage D-part1: Train RVQ Codec
â†’ Learn good codebooks
â†’ Mel â†” Codes conversion

Stage D-part2: Train Talker
â†’ Learn to predict codes
â†’ Use frozen RVQ for targets

Result: Complete text-to-speech system!
```

---

## ğŸ’¡ Key Takeaways

âœ… **Talker** autoregressively generates speech codes  
âœ… **Decoder-only transformer** (4 layers, causal attention)  
âœ… **Two separate heads** for base and residual codes  
âœ… **Generates frame-by-frame** like text generation  
âœ… **Uses RoPE** for positional encoding  
âœ… **KV caching** for efficient generation  
âœ… **Trained with teacher forcing** on RVQ-encoded speech  
âœ… **Works with RVQ + vocoder** to produce audio

---

## ğŸ“ Self-Check Questions

1. Why can the Talker use the same autoregressive approach as text generation?
2. What is the purpose of having separate base and residual prediction heads?
3. Why are base and residual embeddings summed rather than concatenated?
4. What is teacher forcing and why do we use it during training?
5. Why must the RVQ codec be frozen during Talker training?

<details>
<summary>ğŸ“ Click to see answers</summary>

1. Because RVQ converts continuous mel spectrograms into discrete codes. These codes form a finite vocabulary (128 options per codebook), allowing us to use softmax and sampling just like predicting the next word
2. Base and residual codes are predicted independently - each needs its own distribution over 128 possible codes. Separate heads allow the model to learn different prediction strategies for coarse patterns (base) vs fine details (residual)
3. Summing is more parameter-efficient and allows both embeddings to contribute to a single unified token representation. It's standard practice in multi-codebook models and works well empirically
4. Teacher forcing means using ground truth previous codes during training instead of model predictions. This provides stable, correct context and speeds up training by avoiding error accumulation
5. Because we need stable, unchanging code targets during training. If RVQ changes, the codes would be a "moving target" and the Talker couldn't learn effectively. RVQ is pre-trained and frozen to provide consistent targets
</details>

---

[Continue to Chapter 25: Multimodal Fusion â†’](25-multimodal-fusion.md)

**Chapter Progress:** Î¼Omni Components â—â—â—â—â—‹ (4/5 complete)

---

## ğŸ“Š Specifications

| Parameter | Value |
|-----------|-------|
| **Dimension** | 192 |
| **Layers** | 4 |
| **Heads** | 3 |
| **Codebooks** | 2 |
| **Output** | 2 Ã— 128 logits |
| **Parameters** | ~10.1M |
| **max_mel_length** | Auto-calculated from dataset (95th percentile) - for CUDA graphs compatibility |
| **Frame rate** | 12.5 Hz (with frame_ms=80) |

## ğŸ”„ Generation Process

```
1. Start: codes = [[0, 0]]  (start token)

2. Predict next frame:
   base_logits, res_logits = talker(codes)
   base = argmax(base_logits)  # â†’ 42
   res = argmax(res_logits)    # â†’ 87
   codes = [[0,0], [42,87]]

3. Repeat for T frames...

4. Decode with RVQ:
   mel = rvq.decode(codes)

5. Vocode with Griffin-Lim:
   audio = vocoder.mel_to_audio(mel)
```

## ğŸ’¡ Key Takeaways

âœ… **Autoregressive** code prediction  
âœ… **2 separate heads** (base + residual)  
âœ… **Uses KV caching** for speed  
âœ… **Works with RVQ + vocoder** (HiFi-GAN or Griffin-Lim)

---

[Back to Index](00-INDEX.md)

