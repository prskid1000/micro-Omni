# Chapter 02: Neural Networks Fundamentals

[â† Previous: What is AI?](01-what-is-ai.md) | [Back to Index](00-INDEX.md) | [Next: How Neural Networks Learn â†’](03-training-basics.md)

---

## ğŸ¯ Learning Objectives

By the end of this chapter, you will understand:
- What an artificial neuron is and how it works
- How neurons connect to form networks
- Different types of layers in neural networks
- The forward pass: how data flows through a network

---

## ğŸ§  Biological Inspiration

### The Real Neuron

The human brain contains ~86 billion neurons connected by ~100 trillion synapses.

```
Biological Neuron:

    Dendrites          Cell Body         Axon           Synapses
       â†“                   â†“               â†“                â†“
   â”€â”€â”€â”€â”¬â”€â”€â”€â”€          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”       â”€â”€â”€â”€â”€â”€â”€â”€â”€      â”€â”€â”€â”€â”¬â”€â”€â”€â”€
       â”‚              â”‚   â—‹    â”‚              â”‚            â”‚
   â”€â”€â”€â”€â”¼â”€â”€â”€â”€    â†’     â”‚        â”‚    â†’     â”€â”€â”€â”€â”´â”€â”€â”€â”€   â†’  â”€â”€â”¼â”€â”€
       â”‚              â”‚        â”‚                           â”‚
   â”€â”€â”€â”€â”´â”€â”€â”€â”€          â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”€â”€â”€â”€â”´â”€â”€â”€â”€

   Inputs         Processes Info    Transmits      Connects to
                                    Signal         other neurons
```

**How it works:**
1. **Dendrites** receive signals from other neurons
2. **Cell body** processes these signals
3. If total signal exceeds threshold â†’ **fires** signal down axon
4. **Synapses** pass signal to connected neurons

---

## âš¡ The Artificial Neuron (Perceptron)

An artificial neuron mimics this behavior mathematically.

### Structure

```
          xâ‚ â”€â”€â”€wâ‚â”€â”
                   â”‚
          xâ‚‚ â”€â”€â”€wâ‚‚â”€â”¤    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”œâ”€â”€â”€â†’â”‚   Sum   â”‚â”€â”€â”€â”€zâ”€â”€â”€â†’â”‚Activationâ”‚â”€â”€â”€â”€â†’ output
          xâ‚ƒ â”€â”€â”€wâ‚ƒâ”€â”¤    â”‚ Î£(wáµ¢xáµ¢) â”‚         â”‚ f(z)     â”‚
                   â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          b â”€â”€â”€â”€â”˜
         (bias)

Inputs    Weights    Weighted Sum     Activation    Output
```

### Mathematical Formula

For a single neuron:

```
z = wâ‚xâ‚ + wâ‚‚xâ‚‚ + wâ‚ƒxâ‚ƒ + ... + b

output = f(z)
```

Where:
- **xáµ¢** = inputs (features)
- **wáµ¢** = weights (importance of each input)
- **b** = bias (shift/threshold adjustment)
- **f** = activation function

---

## ğŸ’» Concrete Example: Spam Detection Neuron

Let's build a neuron that detects spam emails!

### Inputs (Features)

```python
# Email: "Win free money now!"
x1 = 3  # Number of exclamation marks
x2 = 2  # Number of "money" words (win, money)
x3 = 1  # Contains "free"? (1=yes, 0=no)
```

### Weights (Learned Importance)

```python
w1 = 0.5   # Exclamation marks matter somewhat
w2 = 2.0   # Money words are strong spam signals
w3 = 1.5   # "Free" is a strong indicator
b = -3.0   # Bias (threshold adjustment)
```

### Computation

```python
# Step 1: Weighted sum
z = (w1 * x1) + (w2 * x2) + (w3 * x3) + b
z = (0.5 * 3) + (2.0 * 2) + (1.5 * 1) + (-3.0)
z = 1.5 + 4.0 + 1.5 - 3.0
z = 4.0

# Step 2: Activation (Sigmoid function)
output = 1 / (1 + e^(-z))
output = 1 / (1 + e^(-4.0))
output = 0.98  # ~98% probability of spam!
```

âœ… **Email classified as SPAM!**

---

## ğŸ”¥ Activation Functions

Activation functions add **non-linearity** to the network, allowing it to learn complex patterns.

### Common Activation Functions

#### 1. **Sigmoid**

```
       1 |         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€
         |       â•±
f(z) =   |     â•±
1/(1+eâ»á¶»)|   â•±
       0 |â”€â•±â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        -âˆ      0      +âˆ

Output: (0, 1)
Use: Binary classification, gates in LSTM
```

#### 2. **ReLU (Rectified Linear Unit)**

```
       âˆ |        â•±
         |       â•±
f(z) =   |      â•±
max(0,z) |     â•±
       0 |â”€â”€â”€â”€â•±â”€â”€â”€â”€â”€â”€â”€
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        -âˆ   0      +âˆ

Output: [0, âˆ)
Use: Most hidden layers (fast, simple)
```

#### 3. **GELU (Gaussian Error Linear Unit)**

```
       âˆ |         â•±
         |        â•±
         |       â•±
         |     â•±â•±
       0 |â”€â”€â”€â•±â”€â”€â”€â”€â”€â”€â”€â”€â”€
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        -âˆ   0      +âˆ

Output: (-âˆ, âˆ) but smoother than ReLU
Use: Transformers, modern architectures
```

ğŸ“Œ **Î¼Omni uses**:
- **GELU** in most layers (smooth, effective)
- **SwiGLU** in feedforward layers (advanced variant)

---

## ğŸ—ï¸ Building a Neural Network

A **neural network** is layers of neurons connected together.

### Simple 3-Layer Network

```
INPUT LAYER    HIDDEN LAYER    OUTPUT LAYER

    xâ‚ â—â”€â”€â”€â”€â”€â”€â”€â”€â— hâ‚ â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â— yâ‚
              â•±  â•²  â•± â•²
    xâ‚‚ â—â”€â”€â”€â”€â—â”€â”€â”€â”€â— hâ‚‚ â—â”€â”€â”€â”€â”€â”€â”€â—
          â•±  â•²  â•±  â•± â•²        
    xâ‚ƒ â—â”€â”€â”€â”€â”€â”€â”€â— hâ‚ƒ â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â— yâ‚‚
      
     3 inputs   3 hidden    2 outputs
                neurons
```

**Each connection has a weight!**

---

## ğŸ¯ Types of Layers

### 1. **Dense/Fully Connected Layer**

Every neuron connects to every neuron in the next layer.

```python
# PyTorch example
import torch.nn as nn

layer = nn.Linear(in_features=10, out_features=5)
# Input: 10 neurons â†’ Output: 5 neurons
# Total weights: 10 Ã— 5 = 50 weights + 5 biases = 55 parameters
```

**Used in:** Most neural networks, including Î¼Omni's projectors

---

### 2. **Convolutional Layer**

Slides a filter over input to detect patterns (mainly for images/audio).

```
Input Image:        Filter:         Output (Feature Map):
â”Œâ”€â”€â”¬â”€â”€â”¬â”€â”€â”¬â”€â”€â”      â”Œâ”€â”€â”¬â”€â”€â”        â”Œâ”€â”€â”¬â”€â”€â”¬â”€â”€â”
â”‚  â”‚  â”‚  â”‚  â”‚      â”‚ 1â”‚ 0â”‚        â”‚  â”‚  â”‚  â”‚
â”œâ”€â”€â”¼â”€â”€â”¼â”€â”€â”¼â”€â”€â”¤      â”œâ”€â”€â”¼â”€â”€â”¤        â”œâ”€â”€â”¼â”€â”€â”¼â”€â”€â”¤
â”‚  â”‚â–ˆâ–ˆâ”‚â–ˆâ–ˆâ”‚  â”‚  *   â”‚ 0â”‚ 1â”‚   â†’    â”‚  â”‚â–ˆâ–ˆâ”‚  â”‚
â”œâ”€â”€â”¼â”€â”€â”¼â”€â”€â”¼â”€â”€â”¤      â””â”€â”€â”´â”€â”€â”˜        â”œâ”€â”€â”¼â”€â”€â”¼â”€â”€â”¤
â”‚  â”‚â–ˆâ–ˆâ”‚â–ˆâ–ˆâ”‚  â”‚                     â”‚  â”‚â–ˆâ–ˆâ”‚  â”‚
â”œâ”€â”€â”¼â”€â”€â”¼â”€â”€â”¼â”€â”€â”¤                     â””â”€â”€â”´â”€â”€â”´â”€â”€â”˜
â”‚  â”‚  â”‚  â”‚  â”‚       Detects
â””â”€â”€â”´â”€â”€â”´â”€â”€â”´â”€â”€â”˜       edges/patterns
```

**Used in:** Î¼Omni's Audio Encoder (ConvDown), image processing

---

### 3. **Embedding Layer**

Converts discrete tokens (words, codes) into continuous vectors.

```
Token ID    â†’    Dense Vector (Embedding)

   5        â†’    [0.23, -0.45, 0.67, 0.12, ...]
"cat"       â†’    [0.1, 0.3, -0.2, 0.5, ...]
  42        â†’    [-0.3, 0.8, 0.1, -0.6, ...]

Vocabulary size: 5000 words
Embedding dimension: 256

Parameters: 5000 Ã— 256 = 1,280,000 embeddings
```

**Used in:** Î¼Omni's Thinker (token embeddings), RVQ codebooks

---

### 4. **Normalization Layer**

Stabilizes training by normalizing activations.

```
Before:                After (RMSNorm):
[-100, 50, 200, 10] â†’  [-0.5, 0.3, 1.2, 0.1]

Prevents:
- Exploding gradients (values too large)
- Vanishing gradients (values too small)
```

**Used in:** Î¼Omni uses **RMSNorm** throughout

---

## ğŸ“Š Layer Sizes and Parameters

### Understanding Parameter Count

```python
# Example: Dense layer
input_size = 256
output_size = 512

# Parameters:
weights = 256 Ã— 512 = 131,072
biases = 512
total = 131,584 parameters

# In PyTorch:
layer = nn.Linear(256, 512)
print(sum(p.numel() for p in layer.parameters()))
# Output: 131584
```

### Î¼Omni's Parameter Breakdown

| Component | Approximate Parameters |
|-----------|----------------------|
| Thinker (LLM) | ~60-80M |
| Audio Encoder | ~10-15M |
| Vision Encoder | ~15-20M |
| Talker | ~10-15M |
| RVQ Codec | ~100K |
| **Total** | **~120-140M** |

ğŸ’¡ For comparison:
- GPT-3: 175 **billion** parameters
- LLaMA 7B: 7 **billion** parameters
- Î¼Omni: 140 **million** parameters (1000x smaller!)

---

## ğŸ”„ The Forward Pass

### Data Flow Through a Network

```
Step-by-step example:

1. INPUT: x = [1.0, 2.0, 3.0]
           â†“
2. LAYER 1 (Dense): Wâ‚ Â· x + bâ‚
   â†’ [0.5, -0.3, 0.8, 1.2]
           â†“
3. ACTIVATION: ReLU
   â†’ [0.5, 0.0, 0.8, 1.2]  (negative â†’ 0)
           â†“
4. LAYER 2 (Dense): Wâ‚‚ Â· hâ‚ + bâ‚‚
   â†’ [2.1, 0.7]
           â†“
5. ACTIVATION: Sigmoid
   â†’ [0.89, 0.67]
           â†“
6. OUTPUT: Probabilities for 2 classes
```

### Code Example

```python
import torch
import torch.nn as nn

# Define a simple network
class SimpleNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.layer1 = nn.Linear(3, 4)   # 3 inputs â†’ 4 hidden
        self.act1 = nn.ReLU()
        self.layer2 = nn.Linear(4, 2)   # 4 hidden â†’ 2 outputs
        self.act2 = nn.Sigmoid()
    
    def forward(self, x):
        x = self.layer1(x)     # Dense layer
        x = self.act1(x)       # Activation
        x = self.layer2(x)     # Dense layer
        x = self.act2(x)       # Activation
        return x

# Use it
model = SimpleNet()
input_data = torch.tensor([1.0, 2.0, 3.0])
output = model(input_data)
print(output)  # â†’ tensor([0.89, 0.67])
```

---

## ğŸ¨ Visualizing What Networks Learn

### Layer-by-Layer Learning

For image classification:

```
INPUT IMAGE: Photo of a cat

LAYER 1 (Shallow): Learns basic features
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Edges, lines, curves    â”‚
â”‚  / \ | â€” â—‹               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

LAYER 2 (Middle): Learns combinations
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Textures, patterns      â”‚
â”‚  Fur, stripes, spots     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

LAYER 3 (Deep): Learns parts
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Eyes, ears, nose        â”‚
â”‚  Body parts              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

LAYER 4 (Deeper): Learns objects
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Complete cat face       â”‚
â”‚  Cat body, cat poses     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

OUTPUT: "Cat" (with 95% confidence)
```

---

## ğŸ“ Network Architectures

### Types by Structure

#### 1. **Feedforward Network**

```
Simple one-direction flow:

Input â†’ Hidden â†’ Hidden â†’ Output
  â†“       â†“        â†“        â†“
[Data flows only forward, no loops]
```

#### 2. **Recurrent Network (RNN)**

```
Has loops for sequential data:

     â”Œâ”€â”€â”€â”€â”€â”
     â†“     â”‚
Input â†’ Hidden â†’ Output
        â†‘    â†“
        â””â”€â”€â”€â”€â”˜
[Loops allow memory of previous inputs]
```

#### 3. **Transformer Network** â­

```
Uses attention mechanism (parallel processing):

Input tokens
    â†“
Self-Attention (all tokens interact)
    â†“
Feedforward
    â†“
Output

[Î¼Omni's Thinker uses this!]
```

---

## ğŸ’ª Network Capacity

### Depth vs Width

```
SHALLOW & WIDE:
â”Œâ”€â—â”€â—â”€â—â”€â—â”€â—â”€â—â”€â—â”€â—â”€â—â”€â—â”€â”
â”‚                      â”‚
â””â”€â—â”€â—â”€â—â”€â—â”€â—â”€â—â”€â—â”€â—â”€â—â”€â—â”€â”˜

DEEP & NARROW:
â”Œâ”€â—â”€â—â”€â”
â”œâ”€â—â”€â—â”€â”¤
â”œâ”€â—â”€â—â”€â”¤
â”œâ”€â—â”€â—â”€â”¤
â”œâ”€â—â”€â—â”€â”¤
â”œâ”€â—â”€â—â”€â”¤
â””â”€â—â”€â—â”€â”˜
```

**General Rule:**
- **Deeper** = Can learn more complex, hierarchical patterns
- **Wider** = More capacity within each level
- **Modern trend:** Deep networks (10-100+ layers)

ğŸ“Œ **Î¼Omni's Thinker:** 4 layers, 256 dimensions (tiny by modern standards!)

---

## ğŸ”¢ Parameter Calculation Exercise

Calculate parameters for this network:

```
Layer 1: Linear(100, 50)
Layer 2: Linear(50, 25)
Layer 3: Linear(25, 10)
```

<details>
<summary>ğŸ’¡ Click for solution</summary>

```
Layer 1: (100 Ã— 50) + 50 = 5,050
Layer 2: (50 Ã— 25) + 25 = 1,275
Layer 3: (25 Ã— 10) + 10 = 260

Total: 6,585 parameters
```
</details>

---

## ğŸ’¡ Key Takeaways

âœ… **Artificial neuron** = Weighted sum + activation function  
âœ… **Neural network** = Layers of neurons connected together  
âœ… **Activation functions** add non-linearity (ReLU, GELU, Sigmoid)  
âœ… **Types of layers**: Dense, Convolutional, Embedding, Normalization  
âœ… **Forward pass** = Data flowing through layers to produce output  
âœ… **Deep networks** can learn hierarchical, complex patterns

---

## ğŸ“ Self-Check Questions

1. What are the three main components of an artificial neuron?
2. Why do we need activation functions?
3. What's the difference between a shallow and deep network?
4. How many parameters does a Linear(10, 5) layer have?
5. What type of layer converts token IDs to vectors?

<details>
<summary>ğŸ“ Click to see answers</summary>

1. Inputs (x), Weights (w), Bias (b), and activation function f
2. To add non-linearity, allowing networks to learn complex, non-linear patterns
3. Shallow has few layers; deep has many layers (can learn hierarchical patterns)
4. (10 Ã— 5) + 5 = 55 parameters (50 weights + 5 biases)
5. Embedding layer
</details>

---

## â¡ï¸ Next Steps

Now you know how neural networks are structured. But how do they learn?

[Continue to Chapter 03: How Neural Networks Learn â†’](03-training-basics.md)

Or return to the [Index](00-INDEX.md) to choose a different chapter.

---

**Chapter Progress:** Foundation â—â—â—‹â—‹â—‹ (2/5 complete)

