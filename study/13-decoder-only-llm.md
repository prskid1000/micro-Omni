# Chapter 13: Decoder-Only Language Models

[â† Previous: Vector Quantization](12-quantization.md) | [Back to Index](00-INDEX.md) | [Next: KV Caching â†’](14-kv-caching.md)

---

## ğŸ¯ Learning Objectives

By the end of this chapter, you will understand:
- What decoder-only models are and why they're powerful
- How causal attention works
- Autoregressive generation step-by-step
- Why Î¼Omni uses decoder-only architecture
- Difference between encoder and decoder models

---

## ğŸ“– Understanding Decoder-Only Models

### The Big Picture: Two Ways to Build Language Models

**Analogy: Reading a Book**

```
ENCODER (BERT-style): Reading for Comprehension
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
You read an entire sentence, then answer questions about it.

"The cat sat on the mat"
â†“ Read the WHOLE thing
â†“ Can look back and forward
â†“ Understand the complete meaning

Use case: "What is on the mat?" â†’ "The cat"
Best for: Understanding, classification, question answering

DECODER (GPT-style): Writing a Story
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
You write one word at a time, building as you go.

"The cat sat on ___"
â†“ You can only look at what you've written so far
â†“ Can't peek at future words (they don't exist yet!)
â†“ Predict the next word: "the"

Use case: Text generation, chat, completion
Best for: Generation, conversation, creativity

Î¼Omni uses DECODER (GPT-style)! â­
```

### Why "Decoder-Only"?

```
The name comes from the original Transformer paper which had:
- Encoder: Processes input
- Decoder: Generates output

"Decoder-only" means:
- We only use the decoder part!
- No separate encoder needed
- Just generate, generate, generate!

Famous decoder-only models:
- GPT (all versions)
- LLaMA
- PaLM
- Î¼Omni's Thinker âœ“
```

---

## ğŸ—ï¸ Architecture Deep Dive

### How Decoder-Only Models Work

**Step-by-Step Example:**

```
Task: Complete "The cat sat on ___"

INITIAL STATE:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Input: "The cat sat on"
Tokens: [15, 234, 42, 89]

PROCESSING:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
1. Embed tokens
   â†“
2. Add positional information (RoPE)
   â†“
3. Causal Self-Attention (can only see previous tokens)
   - "The" can only see "The"
   - "cat" can see "The cat"
   - "sat" can see "The cat sat"
   - "on" can see "The cat sat on"
   â†“
4. Feedforward Network (process each position)
   â†“
5. Layer Normalization
   â†“
6. Repeat for multiple layers (e.g., 4 layers)
   â†“
7. Output: Probability distribution over vocabulary

PREDICTION:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Logits for next token:
- "the": 0.45 (45% probability) â† Most likely!
- "a": 0.20 (20%)
- "mat": 0.15 (15%)
- "floor": 0.10 (10%)
- ...

Pick "the" â†’ Output: "The cat sat on the"
```

**The Architecture in Detail:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     INPUT: Token IDs [15, 234, 42]   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Token Embedding Layer             â”‚
â”‚     [15, 234, 42] â†’ [[0.2,...], ...]  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Positional Encoding (RoPE)        â”‚
â”‚     Add position information          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   DECODER BLOCK 1    â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
    â”‚  â”‚ Causal Attn    â”‚  â”‚  â† Can't see future!
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
    â”‚          â†“            â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
    â”‚  â”‚ Feedforward    â”‚  â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   DECODER BLOCK 2    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   DECODER BLOCK 3    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   DECODER BLOCK 4    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Output Linear Layer               â”‚
â”‚     256 dim â†’ 5000 vocab size         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Softmax                           â”‚
â”‚     Convert to probabilities          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     PREDICTION                        â”‚
â”‚     Next token ID: 156                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”‘ Key Feature: Causal Masking

### Understanding "Causal" Attention

**Analogy: Writing an Essay vs Reading an Essay**

```
WRITING (Causal - Decoder):
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
You're writing: "The cat sat on the ___"

When deciding what to write next:
âœ“ You CAN look at: "The cat sat on the"
âœ— You CAN'T look at: Future words (they don't exist yet!)

This is CAUSAL attention - you can only see the PAST!

READING (Bidirectional - Encoder):
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
You're reading: "The cat sat on the mat"

When understanding "sat":
âœ“ You CAN look at: "The cat" (before)
âœ“ You CAN look at: "on the mat" (after)

This is BIDIRECTIONAL attention - you can see EVERYTHING!
```

### The Attention Mask Visualized

```
Attention mask (lower triangular):
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
         The  cat  sat  on  the  mat
The      â–ˆâ–ˆ   â–‘â–‘   â–‘â–‘   â–‘â–‘  â–‘â–‘   â–‘â–‘   â† "The" can only see itself
cat      â–ˆâ–ˆ   â–ˆâ–ˆ   â–‘â–‘   â–‘â–‘  â–‘â–‘   â–‘â–‘   â† "cat" sees "The" and "cat"
sat      â–ˆâ–ˆ   â–ˆâ–ˆ   â–ˆâ–ˆ   â–‘â–‘  â–‘â–‘   â–‘â–‘   â† "sat" sees up to "sat"
on       â–ˆâ–ˆ   â–ˆâ–ˆ   â–ˆâ–ˆ   â–ˆâ–ˆ  â–‘â–‘   â–‘â–‘   â† "on" sees up to "on"
the      â–ˆâ–ˆ   â–ˆâ–ˆ   â–ˆâ–ˆ   â–ˆâ–ˆ  â–ˆâ–ˆ   â–‘â–‘   â† "the" sees up to "the"
mat      â–ˆâ–ˆ   â–ˆâ–ˆ   â–ˆâ–ˆ   â–ˆâ–ˆ  â–ˆâ–ˆ   â–ˆâ–ˆ   â† "mat" sees everything

â–ˆâ–ˆ = Can attend (look at)
â–‘â–‘ = Masked out (can't see)

Shape: Lower triangular matrix
Why: Prevents "cheating" by looking at future tokens!
```

**Why is This Important?**

```
WITHOUT Causal Masking (cheating!):
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Training: "The cat sat on the mat"
When predicting "on":
- Model can see "the mat" (future words!)
- Learns to cheat: "Oh, 'mat' comes later, so 'on' makes sense"

Testing: "The cat sat on ___"
- No future words available!
- Model is confused: "Where's 'mat'? I need it!"
- Performance collapses! âŒ

WITH Causal Masking (no cheating!):
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Training: "The cat sat on the mat"
When predicting "on":
- Model can only see "The cat sat on"
- Learns genuine patterns: "After 'on', what usually comes?"

Testing: "The cat sat on ___"
- Same setup as training!
- Model works perfectly! âœ“

Causal masking ensures training = testing conditions!
```

---

## ğŸ”„ Autoregressive Generation

### What Does "Autoregressive" Mean?

```
AUTO = Self
REGRESSIVE = Using previous outputs as inputs

In simple terms: Use your own output as the next input!

Like a conversation with yourself:
You: "The cat"
You: "sat" (based on "The cat")
You: "on" (based on "The cat sat")
You: "the" (based on "The cat sat on")
...
```

### Generation Process Step-by-Step

```
Goal: Generate "The cat sat on the mat"

STEP 1: Start with prompt
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Input: "The cat"
       [15, 234]
       â†“ Model
Predict: "sat" (token 42)
Output: "The cat sat"

STEP 2: Use previous output as new input
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Input: "The cat sat"
       [15, 234, 42]
       â†“ Model
Predict: "on" (token 89)
Output: "The cat sat on"

STEP 3: Keep going...
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Input: "The cat sat on"
       [15, 234, 42, 89]
       â†“ Model
Predict: "the" (token 15)
Output: "The cat sat on the"

STEP 4: Continue until done
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Input: "The cat sat on the"
       [15, 234, 42, 89, 15]
       â†“ Model
Predict: "mat" (token 156)
Output: "The cat sat on the mat"

STEP 5: Stop condition
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Model predicts [EOS] token (end of sequence)
OR max length reached
â†’ Stop generating!

Final output: "The cat sat on the mat" âœ“
```

---

## ğŸ†š Encoder vs Decoder Comparison

### Detailed Comparison Table

| Feature | Encoder (BERT) | Decoder (GPT/Î¼Omni) |
|---------|----------------|---------------------|
| **Attention** | Bidirectional (see all) | Causal (see past only) |
| **Task** | Understanding | Generation |
| **Training** | Masked LM (fill blanks) | Next-token prediction |
| **Input** | Complete sentence | Partial sentence |
| **Output** | Embeddings/classification | Next token |
| **Use Cases** | Classification, QA, NER | Chat, completion, generation |
| **Examples** | BERT, RoBERTa | GPT, LLaMA, Î¼Omni |

### Visual Comparison

```
ENCODER (BERT):
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Input: "The [MASK] sat on the mat"
       â†“ Bidirectional attention
Output: "cat" (fill in the blank)

Use: Understanding what fits in context

DECODER (GPT/Î¼Omni):
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Input: "The cat sat on the"
       â†“ Causal attention
Output: "mat" (predict next word)

Use: Generate continuation
```

---

## ğŸ’¡ Why Î¼Omni Uses Decoder-Only

```
Reasons for choosing decoder-only:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

âœ… GENERATION TASKS:
   Î¼Omni needs to:
   - Generate text responses
   - Generate speech (RVQ codes)
   â†’ Decoder is perfect for generation!

âœ… SIMPLICITY:
   Encoder-decoder: Two models to train
   Decoder-only: One model to train
   â†’ Simpler architecture, easier to train

âœ… PROVEN EFFECTIVENESS:
   GPT-3, GPT-4, LLaMA all use decoder-only
   â†’ We know it works well!

âœ… UNIFIED PROCESSING:
   Same architecture handles:
   - Text generation
   - Speech code generation
   â†’ Consistent approach across modalities

âœ… INTERACTIVE USE:
   Great for:
   - Chat applications
   - Completion tasks
   - Creative writing
   â†’ Perfect for Î¼Omni's use cases!
```

---

## ğŸ’¡ Key Takeaways

âœ… **Decoder-only** models generate text autoregressively  
âœ… **Causal attention** prevents seeing future tokens (lower triangular mask)  
âœ… **Autoregressive** means using previous outputs as new inputs  
âœ… **One token at a time** generation (sequential process)  
âœ… **Perfect for generation tasks** (text, speech, etc.)  
âœ… **Î¼Omni's Thinker** is decoder-only (GPT-style)  
âœ… **Simpler than encoder-decoder** (one model, not two)

---

## ğŸ“ Self-Check Questions

1. What does "decoder-only" mean?
2. Why do we need causal masking?
3. What is autoregressive generation?
4. What's the difference between encoder and decoder models?
5. Why does Î¼Omni use decoder-only architecture?

<details>
<summary>ğŸ“ Click to see answers</summary>

1. Using only the decoder part of transformers (no separate encoder), which generates outputs one token at a time
2. To prevent the model from "cheating" by seeing future tokens during training, ensuring training matches inference conditions
3. Using the model's own previous outputs as inputs for generating the next output (self-feeding generation loop)
4. Encoder: bidirectional attention for understanding. Decoder: causal attention for generation
5. Because Î¼Omni needs to generate text and speech, and decoder-only architecture is proven effective for generation tasks
</details>

---

[Continue to Chapter 14: KV Caching â†’](14-kv-caching.md)

**Chapter Progress:** Advanced Architecture â—â—‹â—‹â—‹ (1/4 complete)

---

