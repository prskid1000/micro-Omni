# Chapter 13: Decoder-Only Language Models

[â† Previous: Vector Quantization](12-quantization.md) | [Back to Index](00-INDEX.md) | [Next: KV Caching â†’](14-kv-caching.md)

---

## ğŸ¯ Overview

Decoder-only models (GPT-style) generate text autoregressively using causal attention.

## ğŸ—ï¸ Architecture

```
Input: "The cat sat"
â†“
Causal Self-Attention (can only see previous tokens)
â†“  
Feedforward Network
â†“
Output: Predict next token â†’ "on"
```

## ğŸ”‘ Key Features

### Causal Masking
```
Attention mask (lower triangular):
     The  cat  sat  on
The   âœ“    âœ—    âœ—   âœ—
cat   âœ“    âœ“    âœ—   âœ—
sat   âœ“    âœ“    âœ“   âœ—
on    âœ“    âœ“    âœ“   âœ“

Each position can only attend to previous positions
```

### Autoregressive Generation
```
Step 1: Input "The cat" â†’ Predict "sat"
Step 2: Input "The cat sat" â†’ Predict "on"
Step 3: Input "The cat sat on" â†’ Predict "the"
...
```

## ğŸ†š Encoder vs Decoder

| Feature | Encoder (BERT) | Decoder (GPT) |
|---------|----------------|---------------|
| **Attention** | Bidirectional | Causal |
| **Task** | Understanding | Generation |
| **Training** | Masked LM | Next-token prediction |

## ğŸ’¡ Key Takeaways

âœ… **Causal attention** prevents seeing future tokens  
âœ… **Autoregressive** generation one token at a time  
âœ… **Î¼Omni's Thinker** is decoder-only (GPT-style)

---

[Back to Index](00-INDEX.md)

