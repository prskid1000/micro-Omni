# Chapter 43: Mathematical Foundations

[Back to Index](00-INDEX.md)

---

## ðŸŽ¯ Key Mathematical Concepts

### 1. Attention Mechanism

```
Scaled Dot-Product Attention:

Attention(Q, K, V) = softmax(QK^T / âˆšd_k) V

Where:
- Q: Query matrix (n Ã— d_k)
- K: Key matrix (m Ã— d_k)
- V: Value matrix (m Ã— d_v)
- d_k: Key dimension (for scaling)
- softmax: Converts scores to probabilities
```

### 2. RoPE (Rotary Position Embedding)

```
Rotation matrix for position m:

R_m = [cos(mÎ¸)  -sin(mÎ¸)]
      [sin(mÎ¸)   cos(mÎ¸)]

Applied to query/key:
q_m = R_m @ q
k_n = R_n @ k

Dot product encodes relative position:
q_m^T k_n = q^T R_m^T R_n k = q^T R_{m-n} k

Depends only on (m-n)!
```

### 3. Softmax Temperature

```
Standard softmax:
p_i = exp(z_i) / Î£ exp(z_j)

With temperature Ï„:
p_i = exp(z_i/Ï„) / Î£ exp(z_j/Ï„)

Ï„ > 1: More uniform (creative)
Ï„ < 1: More peaked (conservative)
Ï„ = 1: Standard
```

### 4. Cross-Entropy Loss

```
For classification:
L = -Î£ y_i log(Å·_i)

Where:
- y: True distribution (one-hot)
- Å·: Predicted distribution (softmax output)

Minimizing L maximizes likelihood of correct class
```

### 5. Gradient Flow

```
Chain rule for backpropagation:

âˆ‚L/âˆ‚Wâ‚ = âˆ‚L/âˆ‚y Ã— âˆ‚y/âˆ‚hâ‚ƒ Ã— âˆ‚hâ‚ƒ/âˆ‚hâ‚‚ Ã— âˆ‚hâ‚‚/âˆ‚hâ‚ Ã— âˆ‚hâ‚/âˆ‚Wâ‚

Residual connections help:
h_{l+1} = h_l + F(h_l)

âˆ‚h_{l+1}/âˆ‚h_l = 1 + âˆ‚F/âˆ‚h_l

The "+1" ensures gradient flow!
```

## ðŸ’¡ Key Takeaways

âœ… **Attention** = Weighted combination via softmax  
âœ… **RoPE** encodes relative positions via rotation  
âœ… **Temperature** controls sampling randomness  
âœ… **Cross-entropy** measures prediction quality  
âœ… **Residuals** enable training deep networks

---

[Back to Index](00-INDEX.md)

