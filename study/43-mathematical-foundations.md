# Chapter 43: Mathematical Foundations

[â† Previous: Performance Tuning](42-performance-tuning.md) | [Back to Index](00-INDEX.md) | [Next: Research Papers â†’](44-research-papers.md)

---

## ğŸ“ Core Mathematical Concepts

Mathematical foundations underlying Î¼Omni's architecture.

---

## ğŸ¯ Attention Mechanism

### Formula

```
Attention(Q, K, V) = softmax(QK^T / âˆšd_k) V

Where:
- Q (queries): (seq_len, d_k)
- K (keys): (seq_len, d_k)
- V (values): (seq_len, d_v)
- d_k: key dimension (for scaling)
```

### Why It Works

**Dot product similarity:** `QK^T` measures how related each query is to each key

**Scaling:** `/ âˆšd_k` prevents large values in softmax (gradient stability)

**Soft selection:** `softmax()` converts to probabilities (0-1, sum to 1)

**Weighted sum:** Multiply by V to get attended values

---

## ğŸ”„ RoPE (Rotary Position Embedding)

### Formula

```
RoPE(x, m) = [
  [cos(mÎ¸â‚)  -sin(mÎ¸â‚)]   [xâ‚]
  [sin(mÎ¸â‚)   cos(mÎ¸â‚)] Ã— [xâ‚‚]
]

Where:
- m: position index
- Î¸áµ¢ = 10000^(-2i/d): frequency for dimension i
- Rotates embedding by position-dependent angle
```

### Properties

- **Relative positioning:** Naturally encodes relative distances
- **Extrapolation:** Works for sequences longer than training
- **Efficient:** No learned parameters

---

## ğŸ“Š Cross-Entropy Loss

### Formula

```
Loss = -Î£ yáµ¢ log(Å·áµ¢)

For classification:
Loss = -log(Å·_true_class)

Where:
- y: true distribution (one-hot)
- Å·: predicted probabilities (after softmax)
```

### Intuition

- Penalizes low probability on correct class
- Perfect prediction: loss = 0
- Completely wrong: loss = âˆ

---

## ğŸµ CTC Loss (for ASR)

### Formula

```
L_CTC = -log P(y|x)

Where P(y|x) sums over all valid alignments:
P(y|x) = Î£_{Ï€: B(Ï€)=y} Î _t P(Ï€â‚œ|x)

B(Ï€): CTC collapse function (removes blanks, repeated chars)
```

### Why CTC

- **Variable length:** Audio frames â‰  character count
- **No alignment needed:** Automatically finds best alignment
- **Efficient:** Dynamic programming for computation

---

## ğŸ”¢ Layer Normalization

### Formula

```
LayerNorm(x) = Î³ (x - Î¼) / âˆš(ÏƒÂ² + Îµ) + Î²

Where:
- Î¼ = mean(x): mean over features
- ÏƒÂ² = var(x): variance over features
- Î³, Î²: learnable scale and shift
- Îµ: small constant (1e-6) for stability
```

### RMSNorm (Î¼Omni uses this)

```
RMSNorm(x) = x / RMS(x) Ã— Î³

RMS(x) = âˆš(mean(xÂ²))

Simpler, faster, same effect!
```

---

## ğŸ² Softmax Function

### Formula

```
softmax(xáµ¢) = exp(xáµ¢) / Î£â±¼ exp(xâ±¼)

Properties:
- Output: probabilities (0-1, sum to 1)
- Differentiable: enables gradient descent
- Amplifies differences: large xáµ¢ â†’ high probability
```

### Temperature Scaling

```
softmax(x/T) where T > 0

T = 1: standard
T â†’ 0: argmax (deterministic)
T â†’ âˆ: uniform (random)
```

---

## ğŸ“ˆ Gradient Descent

### Formula

```
Î¸â‚œâ‚Šâ‚ = Î¸â‚œ - Î· âˆ‡L(Î¸â‚œ)

Where:
- Î¸: model parameters
- Î·: learning rate
- âˆ‡L: gradient of loss
```

### Adam Optimizer (Î¼Omni uses)

```
mÌ‚â‚œ = Î²â‚mâ‚œ + (1-Î²â‚)gâ‚œ    // momentum
vÌ‚â‚œ = Î²â‚‚vâ‚œ + (1-Î²â‚‚)gâ‚œÂ²   // variance
Î¸â‚œâ‚Šâ‚ = Î¸â‚œ - Î· mÌ‚â‚œ/âˆš(vÌ‚â‚œ + Îµ)

Benefits: Adaptive learning rates, momentum
```

---

## ğŸ’¡ Key Insights

âœ… **Attention:** Weighted averaging based on similarity  
âœ… **RoPE:** Encodes position via rotation  
âœ… **Cross-entropy:** Measures prediction quality  
âœ… **CTC:** Handles variable-length alignment  
âœ… **Normalization:** Stabilizes training  
âœ… **Softmax:** Converts scores to probabilities

---

[Continue to Chapter 44: Research Papers â†’](44-research-papers.md)

---
