# Chapter 36: Optimization Techniques

[â† Previous: Data Preparation](35-data-preparation.md) | [Back to Index](00-INDEX.md) | [Next: Debugging â†’](37-debugging-troubleshooting.md)

---

## âš¡ Performance Optimizations

Key techniques to speed up training and inference in Î¼Omni.

---

## ðŸš€ Training Optimizations

### 1. Mixed Precision (FP16)

**What:** Use 16-bit floats instead of 32-bit  
**Benefit:** 2x faster, 50% less memory  
**Enabled by default** in Î¼Omni

```python
# Automatically uses torch.cuda.amp.autocast
with torch.cuda.amp.autocast():
    output = model(input)
```

### 2. Gradient Accumulation

**What:** Accumulate gradients over multiple batches  
**Benefit:** Simulate larger batch sizes without OOM

```json
{
  "batch_size": 4,
  "gradient_accumulation_steps": 4
  // Effective batch size = 16
}
```

### 3. Gradient Checkpointing

**What:** Trade compute for memory  
**Benefit:** Train larger models on same GPU

```python
model.gradient_checkpointing_enable()
```

### 4. Flash Attention

**What:** Memory-efficient attention implementation  
**Benefit:** 2-4x faster, less memory

```python
# Automatically used if available
pip install flash-attn
```

---

## ðŸŽ¯ Inference Optimizations

### 1. KV Caching

**Essential for generation!** Reuses computed key-value pairs.

**Without KV cache:**
- Generate 100 tokens: ~30 seconds
- Recomputes attention for all previous tokens every step

**With KV cache:**
- Generate 100 tokens: ~3 seconds
- 10x speedup!

### 2. Batch Inference

Process multiple inputs simultaneously:
```python
responses = model.chat_batch([
    "Question 1?",
    "Question 2?",
    "Question 3?"
])
```

### 3. Quantization (INT8)

**What:** Convert weights to 8-bit integers  
**Benefit:** 4x smaller model, faster inference  
**Trade-off:** Slight quality degradation

```python
model = torch.quantization.quantize_dynamic(
    model, {torch.nn.Linear}, dtype=torch.qint8
)
```

---

## ðŸ’¾ Memory Optimizations

### 1. Lazy Dataset Loading

**What:** Load data on-demand instead of pre-loading into RAM  
**Benefit:** Reduces RAM usage by 90%+ for large datasets  
**Status:** âœ… Implemented in all Î¼Omni training scripts (2024 optimization)

**Technical Approach:**
- Uses binary file mode (`'rb'`) for accurate byte offset tracking
- Manually tracks file positions instead of relying on `f.tell()` (which can be unreliable)
- Stores only integer offsets, not actual data content
- Reads specific items on-demand in `__getitem__` using `f.seek()`

**Optimized Datasets (All Training Scripts):**

| Dataset | Files | Optimization | Memory Savings |
|---------|-------|--------------|----------------|
| **TextDataset** | `train_text.py` | File offset indexing | ~8 bytes/line vs full text |
| **ASRDataset** | `train_audio_enc.py` | CSV row offset indexing | ~8 bytes/row vs full dict |
| **TTSDataset** | `train_talker.py` | CSV row offset indexing | ~8 bytes/row vs full dict |
| **ImgCapDataset** | `train_vision.py` | JSON object offset indexing | ~16 bytes/object vs full JSON |
| **MixDataset** | `sft_omni.py` | All three types (text, CSV, JSON) | Combined savings |

**Implementation Details:**

**Text Files:**
```python
# Before: Loads entire file into RAM
self.lines = [l.strip() for l in open(path) if l.strip()]  # Could be GB!

# After: Only stores file positions (integers)
self.line_offsets = []  # Just 8 bytes per line
with open(path, 'rb') as f:
    offset = 0
    while True:
        line_start = offset
        line_bytes = f.readline()
        if not line_bytes:
            break
        if line_bytes.decode('utf-8').strip():
            self.line_offsets.append(line_start)
        offset += len(line_bytes)  # Manual tracking

# Reads on-demand in __getitem__
def __getitem__(self, i):
    with open(self.path, 'rb') as f:
        f.seek(self.line_offsets[i])
        text = f.readline().decode('utf-8').strip()
```

**CSV Files:**
```python
# Before: Loads all rows into RAM
self.rows = []
for r in csv.DictReader(open(csv_path)):
    self.rows.append(r)  # Could be hundreds of MB!

# After: Stores row offsets + header fieldnames
self.row_offsets = []  # Just 8 bytes per row
self.fieldnames = header_line.decode('utf-8').strip().split(',')
# Reads specific row on-demand with proper CSV parsing
```

**JSON Files:**
```python
# Before: Parses entire JSON array into RAM
self.items = json.load(open(manifest))  # Could be large!

# After: Custom JSON parser finds object boundaries
self.item_offsets = []  # 8 bytes per offset
self.item_lengths = []  # 8 bytes per length
# Parses JSON byte-by-byte to find { } boundaries
# Falls back to full load if parsing fails (robust)
```

**Memory Savings Examples:**
- **10M line text file**: ~500MB â†’ ~80MB (6x reduction)
- **1M row CSV**: ~200MB â†’ ~8MB (25x reduction)
- **100K image JSON**: ~50MB â†’ ~1.6MB (30x reduction)
- **Combined (MixDataset)**: All three optimizations applied simultaneously

**Key Benefits:**
- âœ… RAM usage now typically **lower than VRAM** (was opposite before)
- âœ… Can train on systems with limited RAM (8GB+)
- âœ… No code changes needed - automatic in all datasets
- âœ… Robust fallback for malformed JSON files
- âœ… Maintains same training speed (minimal I/O overhead)

### 2. Efficient Tokenizer Training

**What:** Train tokenizers on entire dataset efficiently  
**Benefit:** Train on full dataset with optimized settings

**Implementation:**
- **Plain text files:** Passed directly to SentencePiece (no streaming, no temp files)
- **CSV/JSON files:** Stream extraction to temp file in `data/.temp/` (streams row-by-row/item-by-item to extract text)
- Processes entire dataset (not just samples) efficiently
- Temporary files auto-cleaned after training
- **Always enables `train_extremely_large_corpus=True`:** Uses 64-bit indexing for maximum file size compatibility
- **BPE model type:** Faster than Unigram, good balance of speed and quality
- **Default speed optimization:** `input_sentence_size=10000000` (10M sentences) limits training data for faster training by default
- **Use all data:** Set `input_sentence_size=0` to use entire corpus (slower but uses more data)

**Memory Behavior:**
- **Plain text:** SentencePiece loads entire file into memory during training (no streaming)
- **CSV/JSON extraction:** Streams data extraction (avoids loading structured data into memory), but SentencePiece still loads the extracted temp file into memory
- **Temp files:** Only used for CSV/JSON text extraction, stored in `data/.temp/` and auto-cleaned

**Note:** SentencePiece loads the entire file into memory during training (whether original or extracted temp file). The `train_extremely_large_corpus` flag enables 64-bit indexing (instead of 32-bit) to handle files > 2GB, but doesn't reduce memory usage. Streaming is only used for extracting text from CSV/JSON structured data.

### 3. Resumable Preprocessing

**What:** All preprocessing operations can resume if interrupted  
**Benefit:** No need to restart from beginning if process is stopped

**Resumable Operations:**
- âœ… **Tokenizer training:** SentencePiece handles large files directly (no streaming needed for plain text)
- âœ… **CSV/JSON extraction:** Temp files created in `data/.temp/` (only when needed)
- âœ… **Vocabulary building:** Saves progress every 10K items (vision, OCR)
- âœ… **Token counting:** Saves progress every 10K samples (text, CSV, images)
- âœ… **Training loops:** Already resumable via checkpoints

**Checkpoint Locations:**
- Vocabulary building: `{save_dir}/vocab_build_checkpoint.json`
- Token counting: `{file_path}.token_count_checkpoint.json`
- OCR vocabulary: `{csv_path}.vocab_checkpoint.json`
- All checkpoints auto-cleaned after successful completion

**Related Optimizations:**
- Removed `gc.collect()` calls (Python's GC handles this automatically)
- Removed `torch.cuda.empty_cache()` calls (PyTorch manages CUDA memory efficiently)
- These manual calls were unnecessary and could actually hurt performance

### 4. DataLoader Workers

**What:** Parallel data loading processes  
**Trade-off:** More workers = faster loading but more RAM

```json
{
  "num_workers": 2,  // Default: 2 workers
  // Reduce to 0 or 1 if RAM is limited
}
```

**Recommendation:**
- **High RAM (32GB+)**: `num_workers: 2-4`
- **Medium RAM (16GB)**: `num_workers: 1-2`
- **Low RAM (8GB)**: `num_workers: 0-1`

### 5. Batch Size Tuning

**What:** Adjust batch size based on available memory  
**Benefit:** Maximize GPU utilization without OOM

```json
{
  "batch_size": 4,  // Start conservatively
  "gradient_accumulation_steps": 4  // Simulate batch_size=16
}
```

**Strategy:**
1. Start with `batch_size: 4` (conservative, works on most GPUs)
2. Increase gradually (8, 16, 32) until you hit OOM
3. Use gradient accumulation to simulate larger batches without OOM

---

## ðŸ’¡ Best Practices

âœ… **Always use FP16** for training  
âœ… **Enable KV caching** for generation  
âœ… **Use Flash Attention** if available  
âœ… **Gradient accumulation** for large batches  
âœ… **Lazy loading enabled** by default (no action needed)  
âœ… **Chunked tokenizer training** - automatically streams entire corpus  
âœ… **Resumable preprocessing** - safe to interrupt and resume  
âœ… **Monitor GPU memory** with `nvidia-smi`  
âœ… **Monitor RAM usage** - should be much lower than VRAM now  
âœ… **Reduce num_workers** if RAM is limited

---

[Continue to Chapter 37: Debugging â†’](37-debugging-troubleshooting.md)

---
