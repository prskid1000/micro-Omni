# Chapter 11: Image Processing for AI

[â† Previous: Audio Processing](10-audio-processing.md) | [Back to Index](00-INDEX.md) | [Next: Vector Quantization â†’](12-quantization.md)

---

## ğŸ¯ What You'll Learn

- Image representation basics
- Convolutional operations
- Vision Transformers (ViT)
- Patch-based image encoding
- How Î¼Omni processes images

---

## ğŸ–¼ï¸ Image Representation

### RGB Images

```
Digital image = 3D tensor (Height Ã— Width Ã— Channels)

Example: 224Ã—224 RGB image

Channel layout:
R: [[255, 200, ...],     G: [[100, 150, ...],     B: [[50, 80, ...],
    [180, 220, ...],         [120, 140, ...],         [60, 70, ...],
    ...]                     ...]                     ...]
    
Shape: (224, 224, 3) = 150,528 pixels Ã— 3 values = 451,584 numbers!
```

---

### Normalization

```python
# Raw image: values 0-255
image = PIL.Image.open("photo.jpg")
img_array = np.array(image)  # (H, W, 3) with values [0, 255]

# Normalize to [0, 1]
img_normalized = img_array / 255.0

# Or standardize (mean=0, std=1)
mean = [0.485, 0.456, 0.406]  # ImageNet statistics
std = [0.229, 0.224, 0.225]
img_standardized = (img_normalized - mean) / std

# PyTorch format: (C, H, W)
img_tensor = torch.from_numpy(img_standardized).permute(2, 0, 1)
```

---

## ğŸ”² Convolutional Operations

### Convolution Basics

```
Input image (5Ã—5):      Filter/Kernel (3Ã—3):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”
â”‚ 1 2 3 4 5  â”‚        â”‚ 1 0 1â”‚
â”‚ 6 7 8 9 10 â”‚    *   â”‚ 0 1 0â”‚
â”‚11 12 13 14 15â”‚        â”‚ 1 0 1â”‚
â”‚16 17 18 19 20â”‚        â””â”€â”€â”€â”€â”€â”˜
â”‚21 22 23 24 25â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Convolution operation:
Slide filter over image, compute dot product

Output (3Ã—3 with stride=1):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ?  ?  ?â”‚
â”‚ ?  ?  ?â”‚
â”‚ ?  ?  ?â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Example: Top-left calculation
(1Ã—1)+(2Ã—0)+(3Ã—1)+(6Ã—0)+(7Ã—1)+(8Ã—0)+(11Ã—1)+(12Ã—0)+(13Ã—1) = 32
```

---

### Convolutional Layers

```python
import torch.nn as nn

# 2D Convolution
conv = nn.Conv2d(
    in_channels=3,      # RGB input
    out_channels=64,    # 64 feature maps
    kernel_size=3,      # 3Ã—3 filter
    stride=1,           # Step size
    padding=1          # Preserve spatial size
)

# Input: (B, 3, 224, 224)
# Output: (B, 64, 224, 224)
```

---

## ğŸ¯ Vision Transformer (ViT)

### Patch-Based Approach

```
Traditional CNN:
Image â†’ Conv layers â†’ Features

ViT:
Image â†’ Split into patches â†’ Transformer

Why patches?
âœ… Reduces sequence length
âœ… Captures local patterns
âœ… Compatible with transformers
```

---

### ViT Architecture

```
Image (224Ã—224Ã—3)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Patch Embedding (16Ã—16 patches)     â”‚
â”‚                                      â”‚
â”‚ Image grid: 224/16 = 14 patches     â”‚
â”‚ Total patches: 14Ã—14 = 196          â”‚
â”‚                                      â”‚
â”‚ Each patch: 16Ã—16Ã—3 = 768 values    â”‚
â”‚ Project to d_model dimensions        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Add CLS Token + Position Embeddings â”‚
â”‚                                      â”‚
â”‚ [CLS] [P1] [P2] ... [P196]          â”‚
â”‚   â†“     â†“    â†“         â†“             â”‚
â”‚  +pos  +pos +pos     +pos            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Transformer Encoder Blocks          â”‚
â”‚ (Self-attention + FFN) Ã— L layers   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Extract CLS Token                   â”‚
â”‚ Use as image representation         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Î¼Omni: 14Ã—14 patches, d_model=128, 4 layers
```

---

## ğŸ’» Î¼Omni's Vision Encoder

### ViT-Tiny Implementation

```python
# From omni/vision_encoder.py (simplified)
class ViTTiny(nn.Module):
    def __init__(self, img_size=224, patch=16, d=128, layers=4):
        super().__init__()
        
        # Patch embedding (convolution with stride=patch)
        self.proj = nn.Conv2d(3, d, kernel_size=patch, stride=patch)
        
        # CLS token (learnable)
        self.cls = nn.Parameter(torch.zeros(1, 1, d))
        
        # Position embeddings (learnable)
        num_patches = (img_size // patch) ** 2
        self.pos = nn.Parameter(torch.zeros(1, 1 + num_patches, d))
        
        # Transformer encoder
        self.blocks = nn.ModuleList([
            nn.TransformerEncoderLayer(d, heads=2, dim_feedforward=512)
            for _ in range(layers)
        ])
        
        self.norm = RMSNorm(d)
    
    def forward(self, x):
        # x: (B, 3, 224, 224)
        
        # Patch embedding
        x = self.proj(x)  # (B, d, 14, 14)
        x = x.flatten(2).transpose(1, 2)  # (B, 196, d)
        
        # Add CLS token
        B = x.shape[0]
        cls = self.cls.expand(B, -1, -1)  # (B, 1, d)
        x = torch.cat([cls, x], dim=1)    # (B, 197, d)
        
        # Add position embeddings
        x = x + self.pos
        
        # Transformer blocks
        for block in self.blocks:
            x = block(x)
        
        x = self.norm(x)
        
        # Extract CLS and patch tokens
        cls_token = x[:, 0:1, :]   # (B, 1, d)
        patch_tokens = x[:, 1:, :]  # (B, 196, d)
        
        return cls_token, patch_tokens
```

---

## ğŸ“Š Image Processing Pipeline

### Î¼Omni's Complete Flow

```
1. Load & Resize
   Raw image (any size) â†’ (224, 224, 3)

2. Normalize
   [0, 255] â†’ [0, 1] â†’ standardize

3. To Tensor
   (224, 224, 3) â†’ (3, 224, 224)

4. Patch Embedding
   (3, 224, 224) â†’ (196, 128)

5. Add CLS + Positional
   (196, 128) â†’ (197, 128)

6. Transformer Encoder
   (197, 128) â†’ (197, 128)

7. Extract CLS Token
   (197, 128) â†’ (1, 128)

8. Vision Projector
   (1, 128) â†’ (1, 256)

9. Ready for Thinker!
   Single token representing entire image
```

---

## ğŸ¨ Data Augmentation for Images

### Training-Time Augmentations

```python
from torchvision import transforms

train_transform = transforms.Compose([
    # Geometric
    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),
    transforms.RandomHorizontalFlip(p=0.5),
    transforms.RandomRotation(degrees=15),
    
    # Color
    transforms.ColorJitter(
        brightness=0.2,
        contrast=0.2,
        saturation=0.2,
        hue=0.1
    ),
    
    # Normalize
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                        std=[0.229, 0.224, 0.225])
])
```

---

## ğŸ” Why CLS Token?

### Aggregating Image Information

```
Patch tokens: Local information
[P1]: Top-left corner features
[P2]: Top-middle features
...
[P196]: Bottom-right corner features

CLS token: Global information
Through self-attention, CLS attends to all patches:
CLS â†’ [P1, P2, ..., P196]

Result: CLS aggregates entire image into single vector!

Alternative approaches:
- Average all patch tokens (simple but loses structure)
- Use all patch tokens (too many for Î¼Omni)
- CLS token (efficient aggregation) â† Î¼Omni uses this
```

---

## ğŸ“Š ViT vs CNN Comparison

| Feature | CNN | ViT |
|---------|-----|-----|
| **Processing** | Local receptive field | Global attention |
| **Inductive bias** | Strong (locality) | Weak (learned) |
| **Data efficiency** | Good (small data) | Requires more data |
| **Scalability** | Limited | Excellent |
| **Context** | Hierarchical | Global from start |

```
CNN: Local â†’ Regional â†’ Global (hierarchical)
ViT: Global attention from layer 1 (patch interactions)

Î¼Omni uses ViT (better for multimodal integration)
```

---

## ğŸ’¡ Key Takeaways

âœ… **Images** are 3D tensors (HÃ—WÃ—C)  
âœ… **Convolutions** detect local patterns  
âœ… **ViT** splits images into patches (16Ã—16)  
âœ… **Patch embeddings** reduce sequence length (196 patches)  
âœ… **CLS token** aggregates global image information  
âœ… **Î¼Omni uses ViT-Tiny** (128-dim, 4 layers)  
âœ… **Output**: Single 256-dim vector per image

---

## ğŸ“ Self-Check Questions

1. What are the dimensions of a 224Ã—224 RGB image?
2. How many patches does ViT create from a 224Ã—224 image with patch size 16?
3. What is the CLS token used for?
4. What's the advantage of ViT over traditional CNNs?
5. How many dimensions is Î¼Omni's final image representation?

<details>
<summary>ğŸ“ Answers</summary>

1. (224, 224, 3) or (3, 224, 224) in PyTorch format
2. (224/16) Ã— (224/16) = 14 Ã— 14 = 196 patches
3. CLS token aggregates global image information through self-attention with all patches
4. ViT uses global attention from the start (vs CNN's local receptive fields), better scalability
5. 256 dimensions (after vision projector maps 128â†’256)
</details>

---

[Continue to Chapter 12: Vector Quantization â†’](12-quantization.md)

**Chapter Progress:** Core Concepts â—â—â—â—â—â—â—‹ (6/7 complete)

