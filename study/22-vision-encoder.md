# Chapter 22: Vision Encoder (ViT-Tiny)

[Back to Index](00-INDEX.md)

---

## ğŸ¯ Purpose

Convert images to semantic embeddings for the Thinker.

## ğŸ—ï¸ Architecture

```
Image (3, 224, 224)
    â†“
Patch Embedding (16Ã—16 patches)
  â†’ 14Ã—14 = 196 patches
    â†“
Add CLS Token + Positional Embeddings
  â†’ (197, 128)
    â†“
Transformer Encoder (4 layers)
    â†“
Extract CLS Token
  â†’ (1, 128)
    â†“
Vision Projector: Linear(128 â†’ 256)
    â†“
Ready for Thinker: (1, 256)
```

## ğŸ“Š Specifications

| Parameter | Value |
|-----------|-------|
| **Input** | Image (224Ã—224Ã—3) |
| **Patch Size** | 16Ã—16 |
| **Patches** | 196 + 1 CLS |
| **Dimension** | 128 |
| **Layers** | 4 |
| **Parameters** | ~15-20M |

## ğŸ“ Training

**Task**: Image classification/understanding  
**Loss**: Cross-entropy  
**Data**: Images + captions

## ğŸ’¡ Key Takeaways

âœ… **Vision Transformer** (patch-based)  
âœ… **196 patch tokens + CLS token**  
âœ… **CLS token aggregates** global information  
âœ… **Output**: Single 256-dim vector per image

---

[Back to Index](00-INDEX.md)

