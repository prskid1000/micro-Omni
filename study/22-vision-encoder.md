# Chapter 22: Vision Encoder (ViT-Tiny)

[â† Previous: Audio Encoder](21-audio-encoder.md) | [Back to Index](00-INDEX.md) | [Next: RVQ Codec â†’](23-codec-rvq.md)

---

## ğŸ¯ Learning Objectives

By the end of this chapter, you will understand:
- What the Vision Encoder does and why we need it
- How Vision Transformers (ViT) work
- Patch-based image processing
- The role of the CLS token
- Complete architecture breakdown
- How it connects to the Thinker
- Training process

---

## ğŸ’¡ What is the Vision Encoder?

### The Image Understanding Module

**Analogy: Looking at a Photo Album**

```
Think of processing an image like understanding a photo:

RAW IMAGE PIXELS:
224Ã—224Ã—3 = 150,528 numbers!
â†“
Like seeing: Millions of colored dots
- Too detailed (every pixel!)
- No structure
- Hard to understand meaning

PATCHES (16Ã—16 chunks):
196 patches, each 16Ã—16 pixels
â†“
Like seeing: Small tiles of the image
- Top-left: "orange fur"
- Top-middle: "pointy ears"
- Center: "cat face"
- Bottom: "whiskers"

VISION ENCODER OUTPUT:
Single 256-dim embedding
â†“
Like understanding: "This is a cat"
- Captures MEANING, not just pixels
- Efficient (one vector for whole image!)
- Ready for reasoning (Thinker can use it)

The Vision Encoder is the INTERPRETER:
Pixels â†’ Meaningful understanding!
```

**Why Do We Need This?**

```
Problem: Thinker can't work with raw pixels!
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Raw image issues:
âŒ Too many pixels (224Ã—224Ã—3 = 150,528 numbers!)
âŒ No structure (just RGB values)
âŒ Wrong dimension (need 256, not 150,528!)
âŒ Too low-level (pixels, not concepts)

Solution: Vision Encoder!
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Vision Encoder transforms:
âœ… 150,528 pixels â†’ 1 embedding (massive compression!)
âœ… Low-level pixels â†’ High-level concept
âœ… 3-channel RGB â†’ 256-dim semantic embedding
âœ… Aligns with text/audio embeddings (all 256-dim)

Now Thinker can:
- Process images efficiently
- Understand meaning (not just pixels)
- Combine with text and audio seamlessly!
```

---

## ğŸ—ï¸ Detailed Architecture Breakdown

### The Complete Pipeline

```
INPUT: Cat photo (224Ã—224 pixels, RGB)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Step 1: Start with RGB image
Shape: (3, 224, 224)
- 3 channels (Red, Green, Blue)
- 224Ã—224 pixels
- Total: 150,528 numbers!

Step 2: Divide into patches (16Ã—16)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

WHY patches instead of pixels?
- Processing 150K pixels individually = too slow!
- Patches = natural visual units (like "words" in images)
- 16Ã—16 patch = meaningful visual element

HOW many patches?
- Horizontal: 224 Ã· 16 = 14 patches
- Vertical: 224 Ã· 16 = 14 patches
- Total: 14 Ã— 14 = 196 patches

Each patch:
- Size: (3, 16, 16) = 768 numbers
- Contains: Small piece of image (part of cat ear, nose, etc.)

Visual:
â”Œâ”€â”¬â”€â”¬â”€â”¬â”€â”¬â”€â”¬â”€â”¬â”€â”¬â”€â”¬â”€â”¬â”€â”¬â”€â”¬â”€â”¬â”€â”¬â”€â”
â”œâ”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¤  â† Each square
â”œâ”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¤    is a 16Ã—16
â”œâ”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¤    patch
â”œâ”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¤
â”œâ”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¤  14Ã—14 = 196
â”œâ”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¤  patches total
â”œâ”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¤
â”œâ”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¤
â”œâ”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¤
â”œâ”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¤
â”œâ”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¤
â”œâ”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¤
â”œâ”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¤
â””â”€â”´â”€â”´â”€â”´â”€â”´â”€â”´â”€â”´â”€â”´â”€â”´â”€â”´â”€â”´â”€â”´â”€â”´â”€â”´â”€â”˜

Step 3: Patch Embedding
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Linear projection: (3Ã—16Ã—16) â†’ 128 dimensions

Each patch (768 numbers) â†’ 128-dim vector

Why? Reduce dimensionality for efficient processing!
- 768 numbers per patch â†’ 128 (6x compression)
- Still captures all important visual info

Result: (196, 128)
- 196 patch embeddings
- 128 dimensions each

Step 4: Add CLS Token + Positional Embeddings
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

CLS Token (Classification Token):
- Special learnable token added at the beginning
- Acts as "summary" token
- Will collect information from all patches
- Think: "representative of the entire image"

Positional Embeddings:
- Add position information to each patch
- Patch 0 knows it's top-left
- Patch 195 knows it's bottom-right
- Same concept as in text transformers!

Result: (197, 128)
- 1 CLS token + 196 patch tokens = 197 total
- 128 dimensions each

Layout:
[CLS, patchâ‚€, patchâ‚, patchâ‚‚, ..., patchâ‚â‚‰â‚…]
  â†‘       â†‘                          â†‘
special  top-left                 bottom-right

Step 5: Transformer Encoder (4 layers)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Process with attention:
- All 197 tokens attend to each other
- CLS token gathers info from all patches
- Patches share information with neighbors

4 layers of:
  - Self-attention (tokens talk to each other)
  - Feedforward network (process each token)
  - RMSNorm (stabilize)

After layer 1:
  CLS: "I see some orange and pointy shapes"
  Patch 0: "I'm orange fur"
  Patch 50: "I'm part of an ear"
  ...

After layer 4:
  CLS: "This is a cat!" â† Aggregated understanding
  Patches: Enhanced with global context

Output: (197, 128)

Step 6: Extract CLS Token
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Take only the CLS token (first position):
- CLS has gathered information from ALL patches
- Contains holistic understanding of the image
- Represents entire image in 128 dimensions!

Result: (1, 128)

Discard 196 patch tokens:
- Already served their purpose
- Information aggregated into CLS
- Only need the summary!

Step 7: Vision Projector
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Linear projection: 128 â†’ 256 dimensions

WHY? Align with Thinker's dimension!
- Thinker expects 256-dim embeddings
- Text embeddings: 256-dim
- Audio embeddings: 256-dim
- Image embeddings: 128-dim â†’ 256-dim âœ“

Final output: (1, 256)

READY FOR THINKER! ğŸ‰

One embedding captures the entire image!
```

### Visual Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  INPUT: Cat Photo                       â”‚
â”‚  Shape: (3, 224, 224)                   â”‚
â”‚  RGB image of a cat                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DIVIDE INTO PATCHES                    â”‚
â”‚  16Ã—16 patches                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ [Patch 0] [Patch 1] ... [Patch N]â”‚ â”‚
â”‚  â”‚   16Ã—16      16Ã—16        16Ã—16   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚  Total: 14Ã—14 = 196 patches             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PATCH EMBEDDING                        â”‚
â”‚  Linear projection per patch            â”‚
â”‚  Each (3Ã—16Ã—16) â†’ 128 dims              â”‚
â”‚  Output: (196, 128)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ADD CLS TOKEN & POSITIONAL ENCODING    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ [CLS] + [Patchâ‚€] + ... + [Patchâ‚â‚‰â‚…]â”‚ â”‚
â”‚  â”‚   â†‘         â†‘                â†‘    â”‚ â”‚
â”‚  â”‚ special  top-left      bottom-rightâ”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚  + Positional embeddings                â”‚
â”‚  Output: (197, 128)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TRANSFORMER ENCODER                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Block 1: Attention + FFN + Norm   â”‚ â”‚
â”‚  â”‚  CLS gathers info from patches    â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Block 2: Attention + FFN + Norm   â”‚ â”‚
â”‚  â”‚  Patches share with neighbors     â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Block 3: Attention + FFN + Norm   â”‚ â”‚
â”‚  â”‚  Global understanding emerges     â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Block 4: Attention + FFN + Norm   â”‚ â”‚
â”‚  â”‚  CLS has full image understanding â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚  Output: (197, 128)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  EXTRACT CLS TOKEN                      â”‚
â”‚  Take first token: CLS[0]               â”‚
â”‚  Discard patches (already aggregated)   â”‚
â”‚  Output: (1, 128)                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  VISION PROJECTOR                       â”‚
â”‚  Linear: 128 dim â†’ 256 dim             â”‚
â”‚  Align with Thinker's dimension!       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  OUTPUT: Image Embedding                â”‚
â”‚  Shape: (1, 256)                        â”‚
â”‚  Single token representing "cat"        â”‚
â”‚  Ready for Thinker to process! âœ“        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ” Why Patches? Why Not Pixels?

### The Patch-Based Approach

**Analogy: Reading a Book**

```
PIXEL-BY-PIXEL (reading letter by letter):
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
T-h-e- -c-a-t- -s-a-t- -o-n- -t-h-e- -m-a-t

Problems:
âŒ Too slow (150,528 letters to read!)
âŒ No context (each letter alone is meaningless)
âŒ Expensive (process every single letter)

PATCH-BASED (reading word by word):
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
The cat sat on the mat

Benefits:
âœ… Much faster (6 words vs 18 letters)
âœ… Natural units (words have meaning)
âœ… Efficient (process meaningful chunks)

Same idea for images!
```

**Technical Benefits:**

```
Pixel-level processing (if we tried):
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

224Ã—224 = 50,176 pixels (grayscale)
With RGB: 150,528 numbers

Self-attention on 50K pixels:
- Attention matrix: 50K Ã— 50K = 2.5 billion entries!
- Memory: 10 GB just for one layer!
- Computation: Hours per image!
- Completely impractical! âŒ

Patch-level processing (what we do):
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

196 patches (16Ã—16 each)

Self-attention on 196 patches:
- Attention matrix: 196 Ã— 196 = 38,416 entries
- Memory: ~150 KB per layer
- Computation: Milliseconds per image!
- Completely practical! âœ“

Speed-up: ~256x faster!

Why 16Ã—16 patches work:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Visual reasoning:
- 16Ã—16 = 256 pixels per patch
- Large enough to see meaningful features:
  * Edge of an ear
  * Part of an eye
  * Bit of fur texture
- Small enough to capture details
- Natural "visual word" size

Proven effective:
- ViT (Vision Transformer) uses 16Ã—16
- Beats CNNs on many benchmarks
- Standard in modern vision models!
```

---

## ğŸ¯ The CLS Token: The Aggregator

### Understanding the Special CLS Token

**Analogy: Team Meeting**

```
TEAM MEETING (like transformer attention):
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Attendees:
- Manager (CLS token)
- Engineer 1 (Patch 0 - top-left corner)
- Engineer 2 (Patch 1 - top edge)
- ...
- Engineer 196 (Patch 195 - bottom-right)

Layer 1 (First meeting):
Manager: "Everyone, tell me what you see"
Engineer 1: "I see orange fur"
Engineer 50: "I see a pointy shape (ear?)"
Engineer 100: "I see white whiskers"
Manager: "Hmm, gathering information..."

Layer 2 (Second meeting):
Engineers share with each other too!
Engineer 1 to Engineer 2: "I'm orange, are you?"
Engineer 50 to Engineer 51: "Pointy shape continues here"
Manager: "Okay, getting clearer picture..."

Layer 3 (Third meeting):
More information sharing
Manager: "This is starting to look like an animal"

Layer 4 (Final meeting):
Manager: "Got it! This is definitely a CAT!"

Result: Manager (CLS) has complete understanding!
We only need the manager's summary (CLS token)!
```

**Technical Explanation:**

```
Why CLS token works:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Mechanism:
1. CLS token has NO positional bias
   â†’ Can aggregate from anywhere in image

2. Through attention:
   CLS attends to ALL patches
   â†’ Gathers information from entire image

3. Through layers:
   Layer 1: CLS sees individual patches
   Layer 2: CLS sees patch relationships
   Layer 3: CLS understands regions
   Layer 4: CLS grasps whole image concept

4. Final CLS embedding:
   â†’ Contains holistic understanding
   â†’ "This is a cat with orange fur, pointy ears..."

Alternative approach (without CLS):
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Could average all 196 patch embeddings:
avg = (patchâ‚€ + patchâ‚ + ... + patchâ‚â‚‰â‚…) / 196

Problems:
âŒ Simple averaging loses spatial relationships
âŒ No learned aggregation strategy
âŒ Treats all patches equally (but some more important!)

CLS token approach:
âœ… Learns optimal aggregation through attention
âœ… Can weight important patches more
âœ… Captures spatial relationships
âœ… Proven more effective!
```

---

## ğŸ“Š Detailed Specifications

> **Note**: These are the "tiny" configuration values from `configs/vision_tiny.json`. The code defaults may differ, but config files override them.

### Architecture Parameters

```
PATCH EMBEDDING:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Input: (3, 224, 224)
Patch size: 16Ã—16
Number of patches: 14Ã—14 = 196
Patch flatten: 3Ã—16Ã—16 = 768 dims
Linear projection: 768 â†’ 128 dims
Output: (196, 128)

CLS TOKEN & POSITIONAL EMBEDDING:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
CLS token: Learnable (128 dims)
Positional embeddings: Learnable (197, 128)
Added: CLS + patches + positions
Output: (197, 128)

TRANSFORMER ENCODER:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Dimension: 128
Layers: 4
Attention heads: 2
FFN dimension: 512 (4 Ã— 128)
Dropout: 0.1
Normalization: LayerNorm (standard ViT uses LayerNorm)

VISION PROJECTOR:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Linear: 128 â†’ 256 (no bias)

TOTAL PARAMETERS:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Patch embedding: ~100K
Positional embeddings: ~25K
Transformer blocks: ~14M
Projector: ~33K
Total: ~914K parameters
```

### Comparison Table

| Component | Input | Output | Purpose |
|-----------|-------|--------|---------|
| **Patch Embed** | (3, 224, 224) | (196, 128) | Visual tokenization |
| **Add CLS + Pos** | (196, 128) | (197, 128) | Aggregation + position |
| **Transformer** | (197, 128) | (197, 128) | Visual understanding |
| **Extract CLS** | (197, 128) | (1, 128) | Global representation |
| **Projector** | (1, 128) | (1, 256) | Dimension alignment |

---

## ğŸ“ Training Process

### Pretraining Strategy

**Contrastive Learning (CLIP-style):**

```
Goal: Teach vision encoder to align images with text descriptions

Task: Image-Caption contrastive learning
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Input: Image + Caption pair
Example: [Cat photo] + "A cat sitting on a mat"

This forces the encoder to:
âœ… Learn visual features (edges, textures, shapes)
âœ… Understand objects and parts
âœ… Align visual concepts with text descriptions
âœ… Capture semantic meaning shared between vision and language

Perfect pretraining for multimodal understanding!
Uses trained tokenizer from Stage A for consistent text encoding.
```

**Training Loop:**

```python
for batch in dataloader:
    images, captions = batch  # (B, 3, 224, 224), (B,) list of strings
    
    # 1. Encode images
    cls_output = vit(images)  # (B, 128) - CLS token
    img_emb = img_proj(cls_output)  # (B, embed_dim)
    img_emb = img_emb / img_emb.norm(dim=-1, keepdim=True)  # L2 normalize
    
    # 2. Encode captions (configurable: Thinker or simple embedding)
    text_embs = []
    for caption in captions:
        token_ids = tokenizer.encode(caption)  # Use trained tokenizer
        token_ids = [1] + token_ids[:ctx_len-1]  # Add BOS, truncate
        
        if use_thinker_for_text:
            # Option 1: Use Thinker model (frozen) for contextual embeddings
            token_tensor = torch.tensor(token_ids).unsqueeze(0)  # (1, T)
            with torch.no_grad():
                text_emb = think(idx=token_tensor)  # (1, T, thinker_d_model)
            text_emb = text_emb.squeeze(0).mean(dim=0)  # (thinker_d_model,)
        else:
            # Option 2: Use simple token embeddings
            token_emb = text_embed(torch.tensor(token_ids))  # (T, d_model)
            text_emb = token_emb.mean(dim=0)  # (d_model,)
        
        text_embs.append(text_emb)
    text_embs = torch.stack(text_embs)  # (B, d_model or thinker_d_model)
    text_emb = text_proj(text_embs)  # (B, embed_dim)
    text_emb = text_emb / text_emb.norm(dim=-1, keepdim=True)  # L2 normalize
    
    # 3. Contrastive loss (InfoNCE)
    logits = torch.matmul(img_emb, text_emb.t()) / temperature  # (B, B)
    labels = torch.arange(B, device=device)  # Positive pairs on diagonal
    loss = cross_entropy(logits, labels)
    
    # 4. Backprop and update
    loss.backward()
    optimizer.step()
```

**Key Features:**
- Uses **trained tokenizer** from Stage A (`thinker_ckpt/tokenizer.model`)
- If tokenizer not found, trains new one from image captions
- **Configurable text encoding** via `use_thinker_for_text`:
  - **`true` (recommended)**: Uses frozen Thinker model for contextual embeddings - better quality, aligned with Stage E
  - **`false`**: Uses simple tokenizer + embedding layer - lighter, faster, but less contextual
- **Contrastive learning** aligns image and text embeddings in shared space
- **InfoNCE loss** encourages matching image-caption pairs to be similar

---

## ğŸ”— Connection to Thinker

### How Images Flow into Multimodal Processing

```
COMPLETE PIPELINE:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

1. User uploads cat photo
   Image: (3, 224, 224)
   
2. Vision Encoder processes:
   â†’ Divide into 196 patches
   â†’ Process with transformer
   â†’ Extract CLS token
   â†’ Project to 256-dim: (1, 256)
   
3. User types: "What animal is this?"
   Text tokens: [15, 234, 89, 42, 156]
   â†’ Embed: (5, 256)
   
4. Concatenate:
   Combined input: (6, 256)
   = [1 image token, 5 text tokens]
   
5. Thinker processes:
   â†’ Cross-modal attention
   â†’ Image token interacts with text tokens
   â†’ Understands: User asking about the image
   
6. Generate response:
   Token by token: "This", "is", "a", "cat", "."

Vision encoder enabled visual understanding! âœ“

Efficiency comparison:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Without vision encoder:
- Process 150,528 pixels directly? Impossible!

With vision encoder:
- Process 1 image token! âœ“
- 150,528 â†’ 1 (massive compression)
- All visual information preserved
- Ready for multimodal reasoning
```

---

## ğŸ’¡ Key Takeaways

âœ… **Vision Encoder** translates images into semantic embeddings  
âœ… **Patch-based processing** (16Ã—16 patches) for efficiency  
âœ… **196 patches** from 224Ã—224 image  
âœ… **CLS token** aggregates global image understanding  
âœ… **Transformer encoder** captures visual relationships  
âœ… **Projects to 256-dim** to align with Thinker  
âœ… **Single embedding** represents entire image  
âœ… **~914K parameters** - compact and efficient  
âœ… **Enables multimodal** text+image+audio understanding  
âœ… **Also used in OCR** model for text extraction from images

**Note:** The Vision Encoder (ViT) architecture is also used in the optional OCR model (`train_ocr.py`), where it processes image patches to extract visual features that are then decoded into text sequences.

---

## ğŸ“ Self-Check Questions

1. Why do we use patches instead of processing pixels directly?
2. What is the CLS token and what role does it play?
3. How many patches does a 224Ã—224 image become?
4. Why do we only keep the CLS token and discard the patch tokens?
5. Why project from 128 to 256 dimensions at the end?

<details>
<summary>ğŸ“ Click to see answers</summary>

1. Processing 150K pixels directly would require massive computation (50KÃ—50K attention matrix). Patches (196 total) are much more efficient (196Ã—196) while capturing meaningful visual units
2. CLS token is a special learnable token that aggregates information from all patches through attention. It serves as a global representation of the entire image
3. 224Ã·16 = 14 patches per side, so 14Ã—14 = 196 patches total
4. Through transformer layers, CLS token gathers all relevant information from patches via attention. The final CLS embedding contains the holistic image understanding, so patch tokens are no longer needed
5. To align with Thinker's input dimension (256) - all modalities (text, image, audio) must be 256-dim for unified multimodal processing
</details>

---

[Continue to Chapter 23: RVQ Codec â†’](23-codec-rvq.md)

**Chapter Progress:** Î¼Omni Components â—â—â—‹â—‹â—‹ (2/5 complete)

---

## ğŸ“Š Specifications

| Parameter | Value |
|-----------|-------|
| **Input** | Image (224Ã—224Ã—3) |
| **Patch Size** | 16Ã—16 |
| **Patches** | 196 + 1 CLS |
| **Dimension** | 128 |
| **Layers** | 4 |
| **Parameters** | ~914K |

## ğŸ“ Training

**Task**: Image-Caption contrastive learning (CLIP-style)  
**Loss**: Contrastive loss (InfoNCE)  
**Data**: Images + text captions  
**Text Encoding**: Uses trained tokenizer from Stage A (`thinker_ckpt/tokenizer.model`)

## ğŸ’¡ Key Takeaways

âœ… **Vision Transformer** (patch-based)  
âœ… **196 patch tokens + CLS token**  
âœ… **CLS token aggregates** global information  
âœ… **Output**: Single 256-dim vector per image

---

[Back to Index](00-INDEX.md)

