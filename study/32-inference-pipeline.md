# Chapter 32: Inference Pipeline

[Back to Index](00-INDEX.md)

---

## ðŸŽ¯ Complete Inference Flow

### Text-Only Chat
```
User Input: "What is AI?"
    â†“
Tokenize â†’ [15, 234, 89, 42]
    â†“
Token Embeddings â†’ (1, 4, 256)
    â†“
Thinker (with KV caching)
    â†“
Next token logits â†’ (1, 4, 5000)
    â†“
Argmax â†’ token_id = 156
    â†“
Decode â†’ "AI is..."
    â†“
Repeat until <EOS>
```

### Multimodal (Image + Text)
```
Image + "Describe this"
    â†“
Vision Encoder â†’ (1, 1, 256)
    â†“
Tokenize text â†’ (1, 3, 256)
    â†“
Concatenate â†’ (1, 4, 256)
    â†“
Thinker â†’ Generate response
```

### Text-to-Speech
```
"Hello world"
    â†“
Tokenize (optional conditioning)
    â†“
Talker â†’ RVQ codes (T, 2)
    â†“
RVQ Decode â†’ Mel (T, 128)
    â†“
Griffin-Lim â†’ Audio waveform
```

## âš¡ Optimizations

1. **KV Caching**: Reuse computed K, V
2. **Mixed Precision**: FP16 for speed
3. **Flash Attention**: 2-4x faster
4. **Batch Processing**: Multiple inputs together

## ðŸ’¡ Key Takeaways

âœ… **KV caching** essential for speed  
âœ… **Autoregressive** generation (one token at a time)  
âœ… **Multimodal** handled via concatenation  
âœ… **Multiple output modes** (text, speech)

---

[Back to Index](00-INDEX.md)

