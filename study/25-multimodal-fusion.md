# Chapter 25: Multimodal Fusion Strategy

[â† Previous: The Talker](24-talker-speech-gen.md) | [Back to Index](00-INDEX.md) | [Next: Training Overview â†’](26-training-overview.md)

---

## ğŸ¯ Learning Objectives

By the end of this chapter, you will understand:
- What multimodal fusion means and why it's needed
- Î¼Omni's hybrid fusion strategy
- How different modalities are aligned
- The complete flow from inputs to unified processing
- Token budget and efficiency considerations
- Why this approach enables cross-modal understanding

---

## ğŸ’¡ What is Multimodal Fusion?

### The Integration Challenge

**Analogy: United Nations Meeting**

```
Think of multimodal fusion like a UN meeting:

WITHOUT FUSION (no communication):
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

French delegate: Speaks only French
Chinese delegate: Speaks only Chinese  
Arabic delegate: Speaks only Arabic

Problem: They can't understand each other!
âŒ No communication
âŒ No collaboration
âŒ No unified decision

WITH FUSION (translation + common space):
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Step 1: Each delegate uses interpreter
French â†’ English translator
Chinese â†’ English translator
Arabic â†’ English translator

Step 2: All speak in common language (English)
Now they can:
âœ… Share information
âœ… Discuss together
âœ… Make unified decisions

MULTIMODAL FUSION (same idea!):
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Step 1: Each modality has specialized encoder
Images â†’ Vision Encoder â†’ embeddings
Audio â†’ Audio Encoder â†’ embeddings  
Text â†’ Token Embeddings â†’ embeddings

Step 2: Project to common dimension (256-dim)
All in same "language"!

Step 3: Process together in Thinker
Now they can:
âœ… Attend to each other
âœ… Share information across modalities
âœ… Build unified multimodal understanding!

Fusion is the KEY to multimodal AI!
```

**Why Do We Need This?**

```
Problem: Modalities are fundamentally different
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Images:
- 2D spatial data
- RGB pixels (224Ã—224Ã—3)
- Convolution + attention work well

Audio:
- 1D temporal data  
- Frequency spectrum (TÃ—128)
- Convolution + recurrence work well

Text:
- Discrete symbols
- Token IDs
- Embeddings + attention work well

They're TOO DIFFERENT to process together directly!

Solution: Fusion Strategy
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

1. Use SPECIALIZED encoders for each modality
   â†’ Leverage domain-specific inductive biases
   
2. Project to COMMON embedding space
   â†’ All modalities become sequences of 256-dim vectors
   
3. Process in UNIFIED transformer (Thinker)
   â†’ Cross-modal attention emerges naturally!

Best of both worlds! âœ“
```

---

## ğŸ—ï¸ Î¼Omni's Hybrid Fusion Strategy

### The Two-Stage Approach

**Î¼Omni uses HYBRID fusion:**

```
HYBRID = Specialized Encoding + Unified Processing
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Stage 1: Specialized Encoding
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Each modality gets optimized treatment:

VISION:
- Patch-based processing (16Ã—16 patches)
- Vision Transformer (ViT)
- CLS token aggregation
- Output: (1, 128) embedding

WHY specialized?
âœ… Patches = natural visual units
âœ… ViT = proven for spatial patterns
âœ… CLS = global image representation

AUDIO:
- Mel spectrogram (time-frequency)
- Convolutional downsampling (8x)
- Transformer encoder
- Output: (T_audio/8, 192) embeddings

WHY specialized?
âœ… Mel = human-like frequency perception
âœ… Convolution = local temporal patterns
âœ… Downsampling = efficiency

TEXT:
- Tokenization (subword BPE)
- Embedding lookup
- Output: (T_text, 256) embeddings

WHY specialized?
âœ… BPE = handles all words efficiently
âœ… Direct embedding = simplest for discrete tokens

Stage 2: Alignment + Unified Processing
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Step 1: Project to common dimension (256)
Vision: 128 â†’ 256 (linear projection)
Audio: 192 â†’ 256 (linear projection)
Text: Already 256 âœ“

Step 2: Concatenate all tokens
Combined = [image_tokens, audio_tokens, text_tokens]

Step 3: Process in Thinker
All tokens attend to each other!
Cross-modal understanding emerges!
```

### Visual Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         INPUT: Multiple Modalities          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   IMAGE    â”‚    AUDIO    â”‚       TEXT        â”‚
â”‚  ğŸ–¼ï¸ Cat    â”‚  ğŸ¤ "Meow"  â”‚  ğŸ“ "What is it?" â”‚
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â†“             â†“             â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   VISION    â”‚ â”‚    AUDIO    â”‚ â”‚    TEXT     â”‚
â”‚  ENCODER    â”‚ â”‚   ENCODER   â”‚ â”‚ TOKENIZER   â”‚
â”‚  (ViT)      â”‚ â”‚  (AuT-Tiny) â”‚ â”‚  (BPE)      â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â†“               â†“               â†“
  (1, 128)        (38, 192)        (5, 256)
       â†“               â†“               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚   PROJECT    â”‚ â”‚   PROJECT    â”‚      â”‚
â”‚   128â†’256    â”‚ â”‚   192â†’256    â”‚      â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
       â†“                â†“               â†“
  (1, 256)         (38, 256)       (5, 256)
       â”‚                â”‚               â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚    CONCATENATE        â”‚
            â”‚  Along sequence dim   â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
                (44, 256)
    [1 img, 38 audio, 5 text tokens]
                        â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚     THINKER (Unified)     â”‚
        â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
        â”‚  â”‚  Cross-Modal Attn   â”‚  â”‚
        â”‚  â”‚  Image â†” Audio â†” Textâ”‚  â”‚
        â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
        â”‚                           â”‚
        â”‚  All tokens interact!     â”‚
        â”‚  Understanding emerges!   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   OUTPUT: Text      â”‚
         â”‚ "This is a cat"     â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”„ Complete Flow: Detailed Breakdown

### Step-by-Step Multimodal Processing

```
EXAMPLE: User uploads cat image and asks "What animal is this?"
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

INPUT MODALITIES:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Image: cat.jpg (224Ã—224 RGB)
Text: "What animal is this?"

STEP 1: Process Image
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Vision Encoder:
1. Divide into 196 patches (16Ã—16 each)
2. Embed patches: (196, 128)
3. Add CLS token: (197, 128)
4. 4 transformer layers
5. Extract CLS: (1, 128)
6. Project: 128 â†’ 256
   
Output: (1, 256)
Meaning: "Orange fur, pointy ears, whiskers..."

STEP 2: Process Text
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Tokenizer:
"What animal is this?" â†’ [156, 892, 423, 987, 342]

Embedding:
[156, 892, 423, 987, 342] â†’ (5, 256)

Output: (5, 256)
Meaning: ["What", "animal", "is", "this", "?"]

STEP 3: Align Dimensions
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Check dimensions:
- Image: (1, 256) âœ“
- Text: (5, 256) âœ“

All aligned! Ready to combine!

STEP 4: Concatenate
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Combined input: (6, 256)

Sequence layout:
[img_token, "What", "animal", "is", "this", "?"]
    â†‘         â†‘                                 â†‘
 position 0   position 1                  position 5

STEP 5: Process in Thinker
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Layer 1 Attention:
"What" attends to: [img_token, "What"]
"animal" attends to: [img_token, "What", "animal"]
...
Each text token can SEE the image!

Layer 6 Attention:
Now "animal" has:
- Seen the image features
- Understood "What ... is this?"
- Ready to generate answer!

STEP 6: Generate Response
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Thinker autoregressively generates:

Step 1: Predict next token
Context: [img, "What", "animal", "is", "this", "?"]
Predict: "This" (token 432)

Step 2: Predict next token  
Context: [img, "What", ..., "?", "This"]
Predict: "is" (token 89)

Step 3: Predict next token
Context: [img, "What", ..., "This", "is"]
Predict: "a" (token 56)

Step 4: Predict next token
Context: [img, "What", ..., "is", "a"]
Predict: "cat" (token 781)

Complete response: "This is a cat"

CROSS-MODAL MAGIC! ğŸ‰
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

The model:
âœ… "Saw" the image (orange fur, pointy ears)
âœ… Understood the question (asking for animal type)
âœ… Generated appropriate answer (cat)
âœ… All through unified attention!
```

---

## ğŸ“Š Token Budget & Efficiency

### Managing Sequence Length

**Token Budget Example:**

```
THINKER CAPACITY: 512 tokens max
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Example 1: Image + Short Text
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Image: 1 token (CLS)
Text: "What is this?" = 4 tokens
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Input: 5 tokens
Available for generation: 507 tokens

Plenty of room! âœ“

Example 2: Audio + Text
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Audio (3 seconds):
- 3 sec Ã— 16000 Hz = 48000 samples
- Mel: 48000 / 256 hop = 187 frames
- After 8x downsample: 187 / 8 â‰ˆ 24 tokens

Text: "Transcribe this audio" = 4 tokens
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Input: 28 tokens
Available for generation: 484 tokens

Still plenty! âœ“

Example 3: All Modalities
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Image: 1 token
Audio (5 seconds): ~40 tokens (after downsample)
Text: "Describe what you see and hear" = 7 tokens
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Input: 48 tokens
Available for generation: 464 tokens

Comfortable! âœ“

Example 4: Long Audio
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Audio (30 seconds): ~375 tokens
Text: "Summarize" = 1 token
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Input: 376 tokens
Available for generation: 136 tokens

Getting tight, but manageable!

Why Audio is Expensive:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Audio frame rate: 12.5 Hz (after 8x downsample)
â†’ 12.5 tokens per second
â†’ 1 minute audio = 750 tokens!

This is why we:
1. Downsample aggressively (8x)
2. Use efficient encoders
3. Limit audio duration in practice

Image is Cheap:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Image: Always 1 token (CLS aggregation)
â†’ Any resolution â†’ 1 token!
â†’ Very efficient!

This is why ViT with CLS is powerful!
```

---

## ğŸ¯ Key Principles of Î¼Omni Fusion

### Design Philosophy

**1. Specialized Encoding**

```
PRINCIPLE: Use the right tool for each job
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Vision:
- 2D spatial structure â†’ Patch-based ViT
- Global understanding â†’ CLS token
- Efficient â†’ Single token output

Audio:
- Temporal patterns â†’ Convolutional layers
- Frequency structure â†’ Mel spectrogram
- Efficiency â†’ 8x downsampling

Text:
- Discrete symbols â†’ Tokenization + embeddings
- Already standard â†’ No special encoding needed

Each encoder optimized for its modality! âœ“
```

**2. Common Embedding Space**

```
PRINCIPLE: Speak the same language
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

All modalities â†’ 256 dimensions

WHY 256?
âœ… Large enough for semantic richness
âœ… Small enough for efficiency
âœ… Common standard in transformers

Benefits:
âœ… All tokens can attend to each other
âœ… No special cross-modal attention needed
âœ… Unified processing = simpler architecture
```

**3. Flexible Input**

```
PRINCIPLE: Support any modality combination
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Text only:
input = [text_tokens]

Image + Text:
input = [img_token, text_tokens]

Audio + Text:
input = [audio_tokens, text_tokens]

Image + Audio + Text:
input = [img_token, audio_tokens, text_tokens]

The Thinker doesn't care!
It just sees a sequence of 256-dim vectors!

This flexibility is KEY to multimodal AI!
```

---

## ğŸ’¡ Why This Approach Works

### The Power of Unified Attention

**Cross-Modal Attention Emerges Naturally:**

```
Without explicit cross-modal layers:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Because all modalities are in same sequence,
standard self-attention BECOMES cross-modal!

Example attention pattern:

Token "cat" attending to:
- img_token: 0.4  â† High attention to image!
- "What": 0.1
- "animal": 0.3   â† Relevant word
- "is": 0.05
- "this": 0.15

The model LEARNED to:
âœ… Look at image when generating animal name
âœ… Attend to relevant text context
âœ… Combine multimodal information

No special architecture needed!
Just concatenate + unified attention! ğŸ‰
```

---

## ğŸ’¡ Key Takeaways

âœ… **Hybrid fusion** = Specialized encoders + Unified processing  
âœ… **All modalities** project to common 256-dim space  
âœ… **Concatenation** creates unified sequence  
âœ… **Standard attention** becomes cross-modal  
âœ… **Image: 1 token** (very efficient via CLS)  
âœ… **Audio: ~12.5 tokens/sec** (after 8x downsample)  
âœ… **Text: Variable** based on content  
âœ… **Flexible** - any modality combination works  
âœ… **Emergent** cross-modal understanding through attention

---

## ğŸ“ Self-Check Questions

1. Why does Î¼Omni use specialized encoders instead of processing all modalities the same way?
2. What is the common embedding dimension and why is it important?
3. Why is image input so efficient (only 1 token)?
4. How does cross-modal attention emerge without explicit cross-modal layers?
5. What is the token cost of 10 seconds of audio?

<details>
<summary>ğŸ“ Click to see answers</summary>

1. Different modalities have different structures (2D spatial for images, 1D temporal for audio, discrete for text). Specialized encoders leverage domain-specific inductive biases for better performance
2. 256 dimensions. It's important because all modalities must have the same dimension to be processed together in the unified Thinker. It acts as a "common language"
3. Because Vision Encoder uses a CLS token that aggregates information from all 196 image patches through attention. The entire image is compressed into a single 256-dim vector
4. Because all modality tokens are concatenated into one sequence, standard self-attention naturally allows tokens from different modalities to attend to each other. "Cat" can attend to image_token, enabling cross-modal understanding
5. 10 seconds at 12.5 Hz frame rate = 125 tokens (after the 8x convolutional downsampling in the Audio Encoder)
</details>

---

[Continue to Chapter 26: Training Overview â†’](26-training-overview.md)

**Chapter Progress:** Î¼Omni Components â—â—â—â—â— (5/5 complete!)

---

## ğŸ¯ Key Principles

### 1. Specialized Encoding
- Each modality uses optimized encoder
- Vision: ViT for spatial patterns
- Audio: Conv+Transformer for temporal
- Text: Tokenization + embeddings

### 2. Common Embedding Space
- All project to d_model=256
- Enables cross-modal attention
- Single unified processing

### 3. Flexible Input
```python
# Text only
input = [text_tokens]

# Image + Text
input = [img_token, text_tokens]

# Audio + Text
input = [audio_tokens, text_tokens]

# All modalities
input = [img_token, audio_tokens, text_tokens]
```

## ğŸ“Š Token Budget Example

```
Context: 512 tokens

Image: 1 token (CLS)
Audio (3s): ~38 tokens (at 12.5Hz)
Text prompt: ~20 tokens
---------------------------------
Used: 59 tokens
Available for generation: 453 tokens
```

## ğŸ’¡ Key Takeaways

âœ… **Hybrid fusion** = specialized + unified  
âœ… **All modalities** project to 256-dim  
âœ… **Concatenate** embeddings before Thinker  
âœ… **Cross-modal attention** emerges naturally  
âœ… **Flexible input** (any modality combination)

---

[Back to Index](00-INDEX.md)

