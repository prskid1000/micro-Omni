# Chapter 44: Key Research Papers

[Back to Index](00-INDEX.md)

---

## ðŸ“š Foundational Papers

### Transformers
**"Attention Is All You Need"** (Vaswani et al., 2017)
- Introduced transformer architecture
- Multi-head self-attention
- Position encodings
- [arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)

### GPT (Decoder-Only)
**"Improving Language Understanding by Generative Pre-Training"** (Radford et al., 2018)
- Decoder-only transformers
- Autoregressive generation
- Pre-training + fine-tuning

### Vision Transformer
**"An Image is Worth 16x16 Words"** (Dosovitskiy et al., 2020)
- ViT architecture
- Patch-based image processing
- [arxiv.org/abs/2010.11929](https://arxiv.org/abs/2010.11929)

## ðŸŽ¯ Relevant to Î¼Omni

### RoPE
**"RoFormer: Enhanced Transformer with Rotary Position Embedding"** (Su et al., 2021)
- Rotary position embeddings
- Better extrapolation
- [arxiv.org/abs/2104.09864](https://arxiv.org/abs/2104.09864)

### GQA
**"GQA: Training Generalized Multi-Query Transformer Models"** (Ainslie et al., 2023)
- Grouped query attention
- Efficient inference
- [arxiv.org/abs/2305.13245](https://arxiv.org/abs/2305.13245)

### Multimodal
**"Flamingo: a Visual Language Model"** (Alayrac et al., 2022)
- Multimodal fusion strategies
- Vision-language alignment

**"Qwen-Audio"** (Chu et al., 2023)
- Audio encoder design
- Speech understanding

## ðŸ’¡ Reading Recommendations

**Start with:**
1. Attention Is All You Need (foundational)
2. GPT paper (decoder-only)
3. Vision Transformer (ViT)

**Then explore:**
- RoPE (position encoding)
- GQA (efficiency)
- Multimodal papers (fusion)

---

[Back to Index](00-INDEX.md)

