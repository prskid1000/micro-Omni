# Chapter 14: KV Caching Optimization

[â† Previous: Decoder-Only LLM](13-decoder-only-llm.md) | [Back to Index](00-INDEX.md) | [Next: GQA â†’](15-gqa-attention.md)

---

## ğŸ¯ The Problem

```
Without caching:
Step 1: Process "The" â†’ Compute K, V for "The"
Step 2: Process "The cat" â†’ Recompute K, V for "The" + compute for "cat"
Step 3: Process "The cat sat" â†’ Recompute all K, V again!

Complexity: O(TÂ²) - very slow!
```

## âœ… The Solution: KV Caching

```
With caching:
Step 1: Process "The" â†’ Compute & cache K, V for "The"
Step 2: Process "cat" â†’ Reuse cached "The", compute & cache "cat"
Step 3: Process "sat" â†’ Reuse cached "The cat", compute & cache "sat"

Complexity: O(T) - much faster!
```

## ğŸ’» Implementation

```python
class AttentionWithCache:
    def forward(self, x, cache=None):
        Q, K, V = self.project(x)
        
        if cache is not None:
            # Append new K, V to cached
            K = torch.cat([cache['K'], K], dim=2)
            V = torch.cat([cache['V'], V], dim=2)
        
        # Compute attention
        output = attention(Q, K, V)
        
        # Return output and updated cache
        return output, {'K': K, 'V': V}
```

## ğŸ“Š Speed-up

```
Generation without caching:
100 tokens: ~5.0 seconds

Generation with caching:
100 tokens: ~0.5 seconds

10x faster! âœ“
```

## ğŸ’¡ Key Takeaways

âœ… **KV caching** stores computed keys/values  
âœ… **Speeds up** generation from O(TÂ²) to O(T)  
âœ… **Essential** for interactive applications  
âœ… **Î¼Omni uses KV caching** in Thinker and Talker

---

[Back to Index](00-INDEX.md)

