# Chapter 16: SwiGLU Activation Function

[â† Previous: GQA](15-gqa-attention.md) | [Back to Index](00-INDEX.md) | [Next: MoE â†’](17-mixture-of-experts.md)

---

## ğŸ¯ What is SwiGLU?

**SwiGLU** = Swish-Gated Linear Unit  
A modern activation function used in feedforward layers.

## ğŸ“Š Comparison with Other Activations

### Traditional: ReLU
```python
FFN(x) = W2 Â· ReLU(W1 Â· x)
       = W2 Â· max(0, W1 Â· x)
```

### Modern: GELU
```python
FFN(x) = W2 Â· GELU(W1 Â· x)
```

### SwiGLU (Best!)
```python
FFN(x) = W_down Â· (Swish(W_gate Â· x) âŠ™ W_up Â· x)

Where:
- Swish(x) = x Â· sigmoid(x)
- âŠ™ = element-wise multiplication
- Uses 3 projections (gate, up, down)
```

## ğŸ¨ Visualization

```
Input x
   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   W_gate Â· x    â”‚ â†’ Apply Swish â†’ gate_activated
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   W_up Â· x      â”‚ â†’ up
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
  gate_activated âŠ™ up (element-wise multiply)
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  W_down Â· ...   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
     Output
```

## âœ… Benefits

- âœ… Better gradient flow than ReLU
- âœ… Smoother than GELU
- âœ… Empirically better performance
- âœ… Used in modern LLMs (LLaMA, Qwen, etc.)

## ğŸ’¡ Key Takeaways

âœ… **SwiGLU** = Gated activation with Swish  
âœ… **Better performance** than ReLU/GELU  
âœ… **3 projections** (gate, up, down)  
âœ… **Î¼Omni uses SwiGLU** (optional)

---

[Back to Index](00-INDEX.md)

