# Chapter 18: Normalization Techniques

[â† Previous: MoE](17-mixture-of-experts.md) | [Back to Index](00-INDEX.md) | [Next: Î¼Omni Overview â†’](19-muomni-overview.md)

---

## ğŸ¯ Why Normalize?

**Problem**: Activations can have very different scales  
**Solution**: Normalize to stabilize training

## ğŸ“Š Common Normalization Methods

### 1. LayerNorm (Original Transformer)
```python
mean = x.mean(dim=-1, keepdim=True)
std = x.std(dim=-1, keepdim=True)
normalized = (x - mean) / (std + eps)
output = normalized * gamma + beta
```

### 2. RMSNorm (Modern, Faster) â­
```python
rms = torch.sqrt(x.pow(2).mean(dim=-1, keepdim=True))
normalized = x / (rms + eps)
output = normalized * gamma
```

**Differences**:
- RMSNorm: No mean subtraction, no bias
- ~15% faster than LayerNorm
- Similar performance

## ğŸ’» Implementation

```python
class RMSNorm(nn.Module):
    def __init__(self, dim, eps=1e-6):
        super().__init__()
        self.eps = eps
        self.weight = nn.Parameter(torch.ones(dim))
    
    def forward(self, x):
        rms = torch.sqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        return (x / rms) * self.weight
```

## ğŸ¯ Pre-norm vs Post-norm

### Post-norm (Original)
```
x â†’ Attention â†’ Add & Norm â†’ FFN â†’ Add & Norm
```

### Pre-norm (Modern)
```
x â†’ Norm â†’ Attention â†’ Add â†’ Norm â†’ FFN â†’ Add
```

**Pre-norm** is more stable for deep networks!

## ğŸ’¡ Key Takeaways

âœ… **Normalization** stabilizes training  
âœ… **RMSNorm** is faster than LayerNorm  
âœ… **Pre-norm** is more stable than post-norm  
âœ… **Î¼Omni uses RMSNorm** throughout

---

[Back to Index](00-INDEX.md)

