# Chapter 31: Stage E - Multimodal SFT

[â† Previous: Stage D Talker](30-stage-d-talker.md) | [Back to Index](00-INDEX.md) | [Next: Inference Pipeline â†’](32-inference-pipeline.md)

---

## ğŸ¯ Purpose

Fine-tune all components together for multimodal understanding.

## ğŸ“ Task

**Objective**: Joint training on mixed multimodal batches

```
Batch 1: Image + Text â†’ Text response
Batch 2: Audio + Text â†’ Text response
Batch 3: Text only â†’ Text response
Batch 4: Image + Audio + Text â†’ Text response
```

## ğŸ’» Command

```bash
python sft_omni.py --config configs/omni_sft_tiny.json
```

## ğŸ“Š Configuration

```json
{
  "thinker_ckpt": "checkpoints/thinker_tiny/thinker_best.pt",
  "audio_ckpt": "checkpoints/audio_enc_tiny/audio_enc.pt",
  "vision_ckpt": "checkpoints/vision_tiny/vision.pt",
  
  "freeze_encoders": true,
  "batch_size": 8,
  "num_epochs": 5,
  "learning_rate": 1e-4,
  "warmup_steps": 500,
  
  "data_mix": {
    "text_only": 0.4,
    "image_text": 0.3,
    "audio_text": 0.3
  }
}
```

## ğŸ“ Data Format

```
data/multimodal/
â”œâ”€â”€ text/
â”‚   â””â”€â”€ conversations.json
â”œâ”€â”€ images/
â”‚   â””â”€â”€ image_qa.json
â””â”€â”€ audio/
    â””â”€â”€ audio_qa.json
```

## ğŸ“ˆ Expected Progress

```
Epoch 1/5:
Step 100: loss=2.456 text_acc=45.2%
â†’ Image QA acc: 35.8%
â†’ Audio transcription WER: 25.3%

Epoch 3/5:
Step 450: loss=1.678 text_acc=62.8%
â†’ Image QA acc: 58.3%
â†’ Audio transcription WER: 18.5%

Epoch 5/5:
Final: loss=1.123 accuracy=75.6%
â†’ Image QA acc: 68.9%
â†’ Audio transcription WER: 12.7%
```

## ğŸ“Š Key Metrics

**Text Loss**: Next-token prediction
**Image QA Accuracy**: Visual understanding
**Audio WER**: Speech recognition quality

## ğŸ’¡ Training Strategy

1. **Freeze encoders** (optional)
   - Faster training
   - Focus on projectors + Thinker

2. **Unfreeze all** (optional)
   - Better quality
   - Slower training

3. **Mixed batches**
   - Diverse training signal
   - Better generalization

## ğŸ¯ What Gets Trained?

```
âœ… Thinker (fine-tuned)
âœ… Vision Projector (trained from scratch)
âœ… Audio Projector (trained from scratch)
âŒ Vision Encoder (frozen, optional)
âŒ Audio Encoder (frozen, optional)
```

## ğŸ’¡ Tips

1. **Start with frozen encoders** - faster
2. **Lower learning rate** - fine-tuning, not pretraining
3. **Monitor all modalities** - balanced performance
4. **Curriculum learning** - easier â†’ harder data

## ğŸ“ Output

```
checkpoints/omni_sft_tiny/
â”œâ”€â”€ omni.pt               # Final multimodal model
â”‚   â”œâ”€â”€ thinker           # Fine-tuned Thinker
â”‚   â”œâ”€â”€ proj_v            # Vision projector
â”‚   â””â”€â”€ proj_a            # Audio projector
â””â”€â”€ omni_step_500.pt      # Checkpoints
```

## ğŸ‰ Next Steps

After SFT completes, you have a fully trained multimodal model!

**Ready for inference:**
```bash
python infer_chat.py --ckpt_dir checkpoints/omni_sft_tiny
```

---

[Continue to Chapter 32: Inference Pipeline â†’](32-inference-pipeline.md)

