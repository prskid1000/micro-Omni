# Chapter 19: Î¼Omni System Architecture

[Back to Index](00-INDEX.md) | [Next: The Thinker â†’](20-thinker-llm.md)

---

## ğŸ¯ What You'll Learn

- Complete Î¼Omni system architecture
- How all components work together
- Data flow through the system
- Design philosophy and trade-offs

---

## ğŸ—ï¸ High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Î¼Omni SYSTEM                        â”‚
â”‚                                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   INPUTS     â”‚  â”‚  PROCESSING  â”‚  â”‚   OUTPUTS   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                        â”‚
â”‚   ğŸ–¼ï¸ Image          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        ğŸ“ Text       â”‚
â”‚      â†“             â”‚ Thinker  â”‚          â†‘           â”‚
â”‚   Vision Enc  â”€â”€â”€â”€â†’â”‚  (Core   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚      â†“             â”‚   LLM)   â”‚                      â”‚
â”‚   Project          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        ğŸ”Š Speech     â”‚
â”‚                          â†‘                  â†‘         â”‚
â”‚   ğŸ¤ Audio              â”‚            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚      â†“                  â”‚            â”‚  Talker  â”‚    â”‚
â”‚   Audio Enc  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤            â”‚    +     â”‚    â”‚
â”‚      â†“                  â”‚            â”‚   RVQ    â”‚    â”‚
â”‚   Project               â”‚            â”‚    +     â”‚    â”‚
â”‚                         â”‚            â”‚ Vocoder  â”‚    â”‚
â”‚   ğŸ“ Text               â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚      â†“                  â”‚                            â”‚
â”‚   Tokenizer  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                            â”‚
â”‚                                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ§© Core Components

### 1. **Thinker** (Decoder-Only LLM)

```
Role: Central reasoning engine
Type: Transformer decoder (GPT-style)
Size: 256-dim, 4 layers, 4 heads
Params: ~20.32M

Input: Multimodal tokens (text + image + audio)
Output: Next-token predictions

Key Features:
âœ… Causal attention (autoregressive)
âœ… RoPE positional encoding
âœ… KV caching for fast generation
âœ… Optional GQA, SwiGLU, MoE
```

---

### 2. **Vision Encoder** (ViT-Tiny)

```
Role: Convert images to embeddings
Type: Vision Transformer
Size: 128-dim, 4 layers

Input: Image (224Ã—224Ã—3)
Process: 196 patch tokens + CLS token
Output: CLS token (1, 128)

â†’ Vision Projector (128â†’256)
Final: (1, 256) embedding for Thinker
```

---

### 3. **Audio Encoder** (AuT-Tiny)

```
Role: Convert speech to embeddings
Type: Conv + Transformer encoder
Size: 192-dim, 4 layers

Input: Mel spectrogram (T, 128)
Process: 8x downsample + encoding
Output: Frame embeddings (T/8, 192)

â†’ Audio Projector (192â†’256)
Final: (T/8, 256) embeddings for Thinker
```

---

### 4. **Talker** (Speech Code Predictor)

```
Role: Generate speech codes autoregressively
Type: Transformer decoder
Size: 192-dim, 4 layers

Input: Previous RVQ codes (or start token)
Output: Next frame codes (base + residual)

Works with:
- RVQ Codec (2 codebooks, 128 codes each)
- Griffin-Lim Vocoder (mel â†’ audio)
```

---

### 5. **Projectors**

```
Vision Projector: Linear(128 â†’ 256)
Audio Projector: Linear(192 â†’ 256)

Purpose: Align all modalities to Thinker's dimension
Trainable: Yes (trained during SFT)
```

---

## ğŸ”„ Complete Data Flow

### Input â†’ Processing â†’ Output

```
SCENARIO: Image QA

1. USER INPUT:
   Image: cat_photo.jpg
   Text: "What animal is this?"

2. IMAGE PROCESSING:
   cat_photo.jpg
   â†’ Resize (224Ã—224)
   â†’ Vision Encoder
   â†’ CLS token (1, 128)
   â†’ Vision Projector
   â†’ img_emb (1, 1, 256)

3. TEXT PROCESSING:
   "What animal is this?"
   â†’ Tokenizer: [15, 234, 89, 42, 156]
   â†’ Token Embeddings
   â†’ text_emb (1, 5, 256)

4. FUSION:
   combined = [img_emb, text_emb]
   â†’ Shape: (1, 6, 256)

5. THINKER PROCESSING:
   combined â†’ Thinker (4 transformer blocks)
   â†’ Output logits (1, 6, vocab_size)

6. GENERATION:
   Autoregressive decoding:
   â†’ Next token: "This" (ID: 23)
   â†’ Next token: "is" (ID: 67)
   â†’ Next token: "a" (ID: 12)
   â†’ Next token: "cat" (ID: 234)
   â†’ Next token: "." (ID: 5)
   â†’ Next token: <EOS> (ID: 2)

7. OUTPUT:
   "This is a cat."
```

---

## ğŸ“Š Parameter Breakdown

| Component | Parameters | Percentage |
|-----------|-----------|-----------|
| **Thinker** | ~20.32M | ~79.2% |
| **Audio Encoder** | ~2.05M | ~8.0% |
| **Vision Encoder** | ~914K | ~3.6% |
| **Talker** | ~2.24M | ~8.7% |
| **RVQ Codec** | ~49K | ~0.2% |
| **Projectors** | ~82K | ~0.3% |
| **TOTAL** | **~25.65M** | **100%** |

```
For comparison:
- GPT-3: 175 **billion** parameters (6800x larger!)
- LLaMA-7B: 7 **billion** parameters (270x larger)
- BERT-base: 110 **million** parameters (4.3x larger)
- Î¼Omni: 25.65 **million** parameters âœ“
```

---

## ğŸ¯ Design Philosophy

### 1. **Efficiency First**

```
Goal: Train on single 12GB GPU

Strategies:
âœ… Small vocabulary (5K vs 50K+)
âœ… Compact dimensions (256 vs 768+)
âœ… Fewer layers (4 vs 12-96)
âœ… Efficient attention (Flash Attention)
âœ… KV caching for generation
âœ… Gradient checkpointing
```

---

### 2. **Modularity**

```
Each component trains independently:

Stage A: Thinker (text-only)
Stage B: Audio Encoder (ASR task)
Stage C: Vision Encoder (vision task)
Stage D: Talker + RVQ (speech generation)
Stage E: Joint fine-tuning (multimodal SFT)

Benefits:
âœ… Debug easier (isolate issues)
âœ… Parallel development
âœ… Replace components independently
```

---

### 3. **Educational Clarity**

```
Priority: Understandable > State-of-the-art

Code choices:
âœ… Clear variable names
âœ… Comprehensive comments
âœ… Standard PyTorch (no custom CUDA)
âœ… Minimal dependencies
âœ… Well-structured files

Trade-off: ~5-10% performance for 10x readability
```

---

## ğŸ”— Multimodal Fusion Strategy

### Hybrid Fusion

```
Why not early fusion (concatenate raw inputs)?
âŒ Different modalities have different dimensions
âŒ Loses specialized processing benefits

Why not late fusion (combine predictions)?
âŒ No cross-modal interaction during processing

Î¼Omni's Hybrid Fusion:
1. Specialized encoders per modality
2. Project to common dimension (256)
3. Concatenate embeddings
4. Unified Transformer (Thinker) processes all

Benefits:
âœ… Specialized encoding (best of each modality)
âœ… Cross-modal attention (interaction during processing)
âœ… Flexible (any combination of inputs)
```

---

## ğŸ“ˆ Context Management

### Token Budget

```
Total context: 512-2048 tokens

Allocation example (context=512):
- Image: 1 token (CLS)
- Audio (3s): ~38 tokens (at 12.5Hz)
- Text prompt: ~10 tokens
- Available for generation: 512 - 49 = 463 tokens

Strategies:
1. Truncate audio if too long
2. Sample video frames (1 per second)
3. Prioritize recent text context
4. Use KV caching to extend effective context
```

---

## ğŸ’» Codebase Structure

```
Î¼Omni/
â”œâ”€â”€ omni/                      # Core modules
â”‚   â”œâ”€â”€ thinker.py            # Decoder-only LLM
â”‚   â”œâ”€â”€ audio_encoder.py      # AuT-Tiny
â”‚   â”œâ”€â”€ vision_encoder.py     # ViT-Tiny
â”‚   â”œâ”€â”€ talker.py             # Speech generator
â”‚   â”œâ”€â”€ codec.py              # RVQ + vocoder
â”‚   â”œâ”€â”€ tokenizer.py          # BPE tokenizer
â”‚   â”œâ”€â”€ utils.py              # RMSNorm, RoPE, etc.
â”‚   â””â”€â”€ training_utils.py     # Training helpers
â”‚
â”œâ”€â”€ configs/                   # JSON configs
â”‚   â”œâ”€â”€ thinker_tiny.json
â”‚   â”œâ”€â”€ audio_enc_tiny.json
â”‚   â”œâ”€â”€ vision_tiny.json
â”‚   â”œâ”€â”€ talker_tiny.json
â”‚   â””â”€â”€ omni_sft_tiny.json
â”‚
â”œâ”€â”€ train_text.py             # Stage A training
â”œâ”€â”€ train_audio_enc.py        # Stage B training
â”œâ”€â”€ train_vision.py           # Stage C training
â”œâ”€â”€ train_talker.py           # Stage D training
â”œâ”€â”€ sft_omni.py               # Stage E training
â”‚
â”œâ”€â”€ infer_chat.py             # Inference interface
â””â”€â”€ checkpoints/              # Model weights
    â”œâ”€â”€ thinker_tiny/
    â”œâ”€â”€ audio_enc_tiny/
    â”œâ”€â”€ vision_tiny/
    â”œâ”€â”€ talker_tiny/
    â””â”€â”€ omni_sft_tiny/
```

---

## ğŸš€ Inference Modes

### 1. Text-Only Chat

```python
python infer_chat.py --ckpt_dir checkpoints/thinker_tiny

Input: "What is AI?"
Output: "AI is artificial intelligence..."
```

---

### 2. Image Understanding

```python
python infer_chat.py \
  --ckpt_dir checkpoints/omni_sft_tiny \
  --image cat.jpg \
  --text "Describe this image"

Output: "This is a photo of an orange cat sitting..."
```

---

### 3. Speech Input

```python
python infer_chat.py \
  --ckpt_dir checkpoints/omni_sft_tiny \
  --audio_in speech.wav

Output: Transcription + response
```

---

### 4. Text-to-Speech

```python
python infer_chat.py \
  --ckpt_dir checkpoints/omni_sft_tiny \
  --text "Hello world" \
  --audio_out output.wav

Output: output.wav (synthesized speech)
```

---

## ğŸ’¡ Key Takeaways

âœ… **Î¼Omni** = Tiny multimodal AI (25.65M params)  
âœ… **Thinker** = Central decoder-only LLM (GPT-style)  
âœ… **Specialized encoders** for each modality  
âœ… **Hybrid fusion** via projected embeddings  
âœ… **5-stage training** pipeline (modular)  
âœ… **Fits 12GB GPU** (efficient by design)  
âœ… **Educational focus** (clarity > performance)

---

## ğŸ“ Self-Check Questions

1. What are the 5 main components of Î¼Omni?
2. How many parameters does Î¼Omni have total?
3. What dimension do all modalities project to?
4. What type of transformer is the Thinker (encoder/decoder)?
5. What fusion strategy does Î¼Omni use?

<details>
<summary>ğŸ“ Answers</summary>

1. Thinker (LLM), Vision Encoder, Audio Encoder, Talker, RVQ Codec
2. ~25.65 million parameters
3. 256 dimensions (d_model of Thinker)
4. Decoder-only (autoregressive/causal)
5. Hybrid fusion (specialized encoders â†’ project â†’ unified processing)
</details>

---

[Continue to Chapter 20: The Thinker - Core Language Model â†’](20-thinker-llm.md)

**Chapter Progress:** Î¼Omni Architecture â—â—‹â—‹â—‹â—‹â—‹â—‹ (1/7 complete)

