# Chapter 26: Training Workflow Overview

[â† Previous: Multimodal Fusion](25-multimodal-fusion.md) | [Back to Index](00-INDEX.md) | [Next: Stage A â†’](27-stage-a-thinker.md)

---

## ğŸ¯ Learning Objectives

By the end of this chapter, you will understand:
- Why Î¼Omni uses a 5-stage training pipeline
- The purpose and goal of each training stage
- How modular training works and its benefits
- The dependencies between stages
- Resource requirements and time estimates
- Training strategy and design philosophy

---

## ğŸ’¡ Why 5 Stages? The Training Philosophy

### The Challenge of Multimodal Training

**Analogy: Building a Symphony Orchestra**

```
Think of training Î¼Omni like forming an orchestra:

NAIVE APPROACH (train everything together):
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Gather musicians who've never played before
Give them symphony sheet music
Tell them: "Play Beethoven's 9th!"

Problems:
âŒ Too many things to learn at once
âŒ Can't tell which section is struggling
âŒ Everyone gets confused
âŒ Results: Terrible noise!

STAGED APPROACH (train progressively):
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Stage 1: String section practices alone
Stage 2: Wind section practices alone
Stage 3: Brass section practices alone
Stage 4: Percussion section practices alone
Stage 5: All sections play together!

Benefits:
âœ… Each section masters their part
âœ… Can identify and fix issues per section
âœ… Gradual integration
âœ… Results: Beautiful symphony! âœ“

Î¼Omni TRAINING (same idea!):
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Stage A: Thinker learns text (the foundation)
Stage B: Audio Encoder learns sound (audio section)
Stage C: Vision Encoder learns images (vision section)
Stage D: Talker learns speech generation (speech section)
Stage E: All components work together! (full orchestra)

Progressive, modular, effective! âœ“
```

**Why Not Train Everything Together?**

```
Problems with joint training from scratch:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

1. GRADIENT CONFLICTS:
   - Vision gradient pulls one way
   - Audio gradient pulls another way
   - Text gradient pulls a third way
   - Thinker gets confused: "Which to optimize?"
   - Result: Poor convergence âŒ

2. DEBUGGING NIGHTMARE:
   - Model doesn't work well
   - Is it the Thinker? Vision? Audio? Talker?
   - Can't isolate the problem!
   - Waste days debugging âŒ

3. RESOURCE INTENSIVE:
   - Need ALL data types simultaneously
   - Huge memory footprint
   - Long training time with no checkpoints âŒ

4. UNSTABLE TRAINING:
   - Some components learn faster than others
   - Imbalanced learning
   - Hard to tune learning rates âŒ

Benefits of staged training:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

1. FOCUSED LEARNING:
   - Each component has clear objective
   - No conflicting gradients
   - Stable, predictable convergence âœ“

2. EASY DEBUGGING:
   - Stage B fails? Problem is in Audio Encoder
   - Can fix and retrain just that stage
   - Save tons of development time âœ“

3. RESOURCE EFFICIENT:
   - Train on one modality at a time
   - Smaller memory footprint
   - Can parallelize (train stages simultaneously) âœ“

4. MODULAR DEVELOPMENT:
   - Different people can work on different stages
   - Can reuse components (e.g., swap better Vision Encoder)
   - Flexible experimentation âœ“

This is why Î¼Omni uses 5 stages!
```

---

## ğŸ—ï¸ The 5-Stage Training Pipeline

### Complete Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STAGE A: Thinker Pretraining               â”‚
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•   â”‚
â”‚ Purpose: Learn language understanding      â”‚
â”‚ Model: Thinker (decoder-only LLM)          â”‚
â”‚ Task: Predict next word                    â”‚
â”‚ Data: Text corpus (books, articles)        â”‚
â”‚ Loss: Cross-entropy (next token)           â”‚
â”‚ Metric: Perplexity (lower = better)        â”‚
â”‚ Time: ~8-12 hours on 12GB GPU              â”‚
â”‚ Output: thinker_checkpoints/               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â†“
         Foundation is ready!
                   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STAGE B: Audio Encoder Pretraining         â”‚
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•   â”‚
â”‚ Purpose: Learn audio understanding         â”‚
â”‚ Model: Audio Encoder (AuT-Tiny)            â”‚
â”‚ Task: Speech recognition (ASR)             â”‚
â”‚ Data: Audio + transcriptions               â”‚
â”‚ Loss: CTC (alignment-free)                 â”‚
â”‚ Metric: WER (Word Error Rate)              â”‚
â”‚ Time: ~6-10 hours                          â”‚
â”‚ Output: audio_encoder_checkpoints/         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â†“
                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STAGE C: Vision Encoder Training           â”‚
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•   â”‚
â”‚ Purpose: Learn visual understanding        â”‚
â”‚ Model: Vision Encoder (ViT-Tiny)           â”‚
â”‚ Task: Image classification                 â”‚
â”‚ Data: Images + labels                      â”‚
â”‚ Loss: Cross-entropy (classification)       â”‚
â”‚ Metric: Accuracy                           â”‚
â”‚ Time: ~4-8 hours                           â”‚
â”‚ Output: vision_encoder_checkpoints/        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â†“
                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STAGE D: Talker + RVQ Codec Training       â”‚
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•   â”‚
â”‚ Purpose: Learn speech generation           â”‚
â”‚ Models: RVQ Codec + Talker                 â”‚
â”‚ Task: Predict speech codes                 â”‚
â”‚ Data: Speech audio files                   â”‚
â”‚ Loss: MSE (RVQ) + Cross-entropy (Talker)   â”‚
â”‚ Metric: Reconstruction quality            â”‚
â”‚ Time: ~10-15 hours                         â”‚
â”‚ Output: rvq_codec/ + talker_checkpoints/   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â†“
      All components ready!
                   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STAGE E: Multimodal SFT                    â”‚
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•   â”‚
â”‚ Purpose: Teach multimodal understanding    â”‚
â”‚ Models: ALL (Thinker + Encoders)           â”‚
â”‚ Task: Answer multimodal queries            â”‚
â”‚ Data: Image+text, audio+text pairs         â”‚
â”‚ Loss: Cross-entropy (response generation)  â”‚
â”‚ Metric: Task accuracy                      â”‚
â”‚ Time: ~6-12 hours                          â”‚
â”‚ Output: omni_final/                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â†“
      Î¼Omni is ready! ğŸ‰
```

### Detailed Stage Breakdown

```
STAGE A: The Foundation (Text-Only)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Goal: Thinker must understand language before multimodal

What it learns:
- Grammar and syntax
- Common sense reasoning
- World knowledge
- Next token prediction

Think: Teaching reading before showing pictures

STAGE B: Understanding Sound (Audio)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Goal: Audio Encoder converts speech â†’ meaningful embeddings

What it learns:
- Phonemes and words from audio
- Temporal patterns in speech
- Acoustic features
- Alignment between audio and text (via CTC)

Think: Teaching listening comprehension

STAGE C: Understanding Sight (Vision)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Goal: Vision Encoder converts images â†’ meaningful embeddings

What it learns:
- Objects and their features
- Spatial relationships
- Visual patterns
- Semantic understanding of images

Think: Teaching visual recognition

STAGE D: Learning to Speak (Speech Generation)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Goal: System can generate speech (text-to-speech)

What it learns:
Part 1 (RVQ Codec):
- How to discretize mel spectrograms
- Codebook patterns for speech

Part 2 (Talker):
- How to predict speech codes autoregressively
- Prosody and rhythm

Think: Teaching speaking/pronunciation

STAGE E: Bringing It All Together (Multimodal)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Goal: All components work together for cross-modal understanding

What it learns:
- How image relates to text description
- How audio relates to transcription
- Cross-modal reasoning
- Answering questions about images/audio

Think: Teaching to understand and respond to any input

This progressive approach ensures stable, effective learning!
```

---

## ğŸ“Š Training Summary Table

### Complete Specifications

| Stage | Component | Primary Task | Data Type | Loss Function | Metric | Est. Time | Dependencies |
|-------|-----------|--------------|-----------|---------------|---------|-----------|--------------|
| **A** | Thinker | Language Modeling | Text | Cross-Entropy | Perplexity | 8-12h | None |
| **B** | Audio Encoder | ASR | Audio + Text | CTC | WER | 6-10h | None |
| **C** | Vision Encoder | Classification | Images + Labels | Cross-Entropy | Accuracy | 4-8h | None |
| **D** | RVQ + Talker | Speech Gen | Audio (TTS) | MSE + CE | Recon Error | 10-15h | None (RVQ), Then Talker needs RVQ |
| **E** | All (Joint) | Multimodal QA | Mixed Modalities | Cross-Entropy | Task Acc | 6-12h | A, B, C, D |

**Total Estimated Time: 40-60 hours** on single 12GB GPU (tiny model, 25.65M params)

**Note:** Training time scales with model size. See "Model Scaling" section below for larger models.

---

## ğŸ¯ Training Strategy & Design Philosophy

### 1. Modularity

**Principle: Each stage is independent**

```
Why modularity matters:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Debugging:
- Stage C fails? Only rerun Stage C
- Save 30+ hours of retraining!
- Pinpoint issues quickly

Development:
- Multiple people work on different stages
- Parallel development possible
- Faster iteration

Experimentation:
- Want better Vision Encoder?
- Just retrain Stage C and E
- No need to retrain A, B, D

Flexibility:
- Can swap components easily
- Modular design = future-proof
```

### 2. Efficiency

**Principle: Maximize learning with minimal resources**

```
Resource Optimizations:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Small Datasets:
- < 5GB per modality
- Synthetic data where needed
- Enough for proof-of-concept

Single GPU:
- 12GB VRAM sufficient
- Gradient accumulation for larger batches
- Mixed precision (FP16) saves memory

Memory Tricks:
- Gradient checkpointing
- Small batch sizes (2-4)
- Frozen components when appropriate

Time Management:
- Each stage < 15 hours
- Total project: 2-3 days on single GPU
- Feasible for research/prototyping
```

### 3. Progressive Learning

**Principle: Simple to complex**

```
Learning Progression:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Stage A: Text-only (simplest)
  â†“
Stages B, C, D: Individual modalities
  â†“
Stage E: Multimodal (most complex)

Why this works:
âœ… Strong foundation first (text)
âœ… Specialized skills next (vision/audio/speech)
âœ… Integration last (multimodal)

Like learning to walk before you run!
```

---

## âš™ï¸ Before Training: Update Configs Based on Dataset Size

**Important:** After downloading your datasets, update training parameters based on actual data size:

```bash
# Analyze datasets and automatically update all config files
python scripts/update_configs_from_data.py

# Dry run (see what would change without modifying files)
python scripts/update_configs_from_data.py --dry-run
```

**What it does:**
- âœ… Counts samples in production/synthetic datasets
- âœ… Calculates appropriate `max_steps`, `max_epochs`, `warmup_steps`
- âœ… Adjusts `val_freq` and `checkpoint_freq` based on dataset size
- âœ… Updates data paths to production files if they exist
- âœ… Uses best practices: 1-3 epochs for large datasets, 5-10 for small

**When to run:**
- After downloading production datasets
- After generating synthetic test data
- When switching between different dataset sizes

See [Chapter 34: Configuration Files](34-configuration-files.md) for details.

---

## ğŸ“ˆ Model Scaling

### Current Configuration (Tiny)

**Total: 25.65M parameters** - Fits on 12GB GPU

| Component | Config | Parameters |
|-----------|--------|------------|
| **Thinker** | d_model=256, n_layers=4, n_heads=4, d_ff=1024 | 20.32M |
| **Audio Encoder** | d_model=192, n_layers=4, n_heads=3, d_ff=768 | 2.05M |
| **Vision Encoder** | d_model=128, n_layers=4, n_heads=2, d_ff=512 | 914K |
| **Talker** | d_model=192, n_layers=4, n_heads=3, d_ff=768 | 2.24M |

### Scaling to Larger Models

**Moderate Scale (100-200M params):**
- **GPU:** 24GB VRAM
- **Changes:** 2x dimensions, 2x layers
- **Example Thinker:** d_model=512, n_layers=8, n_heads=8, d_ff=2048
- **Training Time:** ~80-120 hours
- **Use Case:** Better quality while staying accessible

**Large Scale (500M-1B params):**
- **GPU:** 40GB+ VRAM (A100) or Multi-GPU
- **Changes:** 3-4x dimensions, 4x layers
- **Example Thinker:** d_model=768, n_layers=16, n_heads=12, d_ff=3072
- **Training Time:** ~200-400 hours
- **Use Case:** Production-quality performance

**Very Large Scale (1B-7B params):**
- **GPU:** Multi-GPU (4-8x A100) or TPU
- **Changes:** 4x dimensions, 8x layers, enable MoE
- **Example Thinker:** d_model=1024, n_layers=32, n_heads=16, d_ff=4096, use_moe=true
- **Training Time:** ~1000+ hours
- **Use Case:** Research, SOTA performance

### Key Parameters to Scale

| Parameter | Impact | Scaling Rule |
|-----------|--------|--------------|
| **d_model** | Quadratic on attention, linear on FFN | 2x d_model â‰ˆ 4x params |
| **n_layers** | Linear increase | 2x layers = 2x params |
| **d_ff** | Linear on FFN | Usually 4x d_model |
| **n_heads** | Minimal | Usually d_model / 64 |
| **vocab_size** | Only embedding layer | Linear increase |

### Memory Requirements

**Training Memory Formula:**
```
Memory â‰ˆ 4 Ã— (model_params Ã— 4 bytes) + (batch_size Ã— ctx_len Ã— d_model Ã— 4 bytes)
```

**Examples:**
- **Tiny (25.65M):** ~12GB VRAM âœ“
- **Moderate (150M):** ~24GB VRAM âœ“
- **Large (700M):** ~40GB+ VRAM (A100)
- **Very Large (3B):** Multi-GPU required

### Scaling Process

1. **Create new config files:**
   ```bash
   cp configs/thinker_tiny.json configs/thinker_medium.json
   # Edit parameters in new config
   ```

2. **Adjust training parameters:**
   - Reduce `batch_size` if OOM
   - Increase `gradient_accumulation_steps`
   - Increase `max_steps` and `warmup_steps`
   - Always use `use_amp: true`, `use_flash: true`

3. **Update projector dimensions** (for Stage E):
   - When scaling Thinker's d_model, update projectors in `sft_omni.py`
   - Vision: `Linear(128 â†’ new_d_model)`
   - Audio: `Linear(192 â†’ new_d_model)`

4. **Train with new configs:**
   ```bash
   python train_text.py --config configs/thinker_medium.json
   # ... repeat for all stages
   ```

### Important Considerations

- **Memory Management:** Use gradient checkpointing, reduce batch size, use gradient accumulation
- **Training Time:** Larger models need 10-100x more training time
- **Data Requirements:** Larger models may need millions more samples per modality
- **Learning Rate:** Consider scaling: `lr = base_lr * sqrt(d_model / 256)`

### Recommended Scaling Path

1. **Start Small:** Get tiny model (25.65M) working perfectly
2. **Scale to Medium (100-200M):** Test quality improvements with 24GB GPU
3. **Evaluate:** Is quality good enough?
4. **If Not:** Scale to Large (500M-1B) with multi-GPU
5. **Production:** Consider 1B-7B for real applications

**Quick Reference:**

| Scale | Total Params | VRAM | Training Time |
|-------|--------------|------|---------------|
| **Tiny** | 25.65M | 12GB | 40-60 hours |
| **Medium** | ~150M | 24GB | 80-120 hours |
| **Large** | ~700M | 40GB+ | 200-400 hours |
| **XL** | ~3B | Multi-GPU | 1000+ hours |

---

## ğŸ“Š Scale vs Performance Analysis

### Performance Scaling with Model Size

**Model Size vs Performance (Quality):**
```
Performance Score (Normalized)
100% â”‚                                    â•±â”€â”€â”€â”€â”€ Plateau
     â”‚                                 â•±â”€
 90% â”‚                              â•±â”€
     â”‚                           â•±â”€
 80% â”‚                        â•±â”€
     â”‚                     â•±â”€
 70% â”‚                  â•±â”€
     â”‚               â•±â”€
 60% â”‚            â•±â”€
     â”‚         â•±â”€
 50% â”‚      â•±â”€
     â”‚   â•±â”€
 40% â”‚â•±â”€
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
       25M   100M   500M   1B    3B    7B
              Model Size (Parameters)

Key Findings:
- 25M (Tiny): ~40-50% of max performance
- 100M (Medium): ~70-80% of max performance  
- 500M (Large): ~85-90% of max performance
- 1B+: ~90-95% of max performance (diminishing returns)

Research: Models under 15B params can achieve 90% of 
larger model performance on many tasks.
```

**Model Size vs Training Time:**
```
Training Time (Hours)
1000+ â”‚                                    â•±â”€â”€â”€â”€â”€
      â”‚                                 â•±â”€
  500 â”‚                              â•±â”€
      â”‚                           â•±â”€
  200 â”‚                        â•±â”€
      â”‚                     â•±â”€
  100 â”‚                  â•±â”€
      â”‚               â•±â”€
   50 â”‚            â•±â”€
      â”‚         â•±â”€
   20 â”‚      â•±â”€
      â”‚   â•±â”€
   10 â”‚â•±â”€
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        25M   100M   500M   1B    3B    7B
              Model Size (Parameters)

Note: Training time scales roughly linearly with parameters,
but larger models need more data and steps.
```

**Model Size vs Inference Speed:**
```
Tokens per Second (TPS)
 100 â”‚â•±â”€â”€â”€â”€â”€
     â”‚   â•²â”€
   50 â”‚      â•²â”€
     â”‚         â•²â”€
   20 â”‚            â•²â”€
     â”‚               â•²â”€
   10 â”‚                  â•²â”€
     â”‚                     â•²â”€
    5 â”‚                        â•²â”€
     â”‚                           â•²â”€
    2 â”‚                              â•²â”€
     â”‚                                 â•²â”€â”€â”€â”€â”€
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
       25M   100M   500M   1B    3B    7B
              Model Size (Parameters)

Inference Speed (12GB GPU, batch_size=1):
- 25M: ~50-100 TPS
- 100M: ~20-40 TPS
- 500M: ~5-10 TPS
- 1B+: <5 TPS (needs larger GPU or quantization)
```

### Expected Performance Benchmarks

**Text Understanding (Perplexity):**
| Model Size | Perplexity | Quality |
|------------|------------|---------|
| 25M (Tiny) | ~30-40 | Basic |
| 100M (Medium) | ~20-25 | Good |
| 500M (Large) | ~15-20 | Very Good |
| 1B+ | ~10-15 | Excellent |

*Lower perplexity = Better*

**Multimodal Understanding (Task Accuracy):**
| Model Size | Image QA | Audio ASR | VQA Score |
|------------|----------|-----------|-----------|
| 25M (Tiny) | ~60% | ~70% | ~55% |
| 100M (Medium) | ~75% | ~85% | ~70% |
| 500M (Large) | ~85% | ~92% | ~80% |
| 1B+ | ~90%+ | ~95%+ | ~85%+ |

*Note: Actual performance depends on training data quality and duration*

### Diminishing Returns Analysis

```
Performance Gain per 2x Parameters
 100% â”‚â•±â”€â”€â”€â”€â”€
      â”‚   â•²â”€
  50% â”‚      â•²â”€
      â”‚         â•²â”€
  25% â”‚            â•²â”€
      â”‚               â•²â”€
  10% â”‚                  â•²â”€
      â”‚                     â•²â”€
   5% â”‚                        â•²â”€â”€â”€â”€â”€
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        25M   100M   500M   1B    3B    7B
              Model Size (Parameters)

Key Insight: Each 2x increase in parameters gives:
- 25Mâ†’50M: ~30% performance gain
- 100Mâ†’200M: ~15% performance gain
- 500Mâ†’1B: ~8% performance gain
- 1Bâ†’2B: ~4% performance gain

Diminishing returns become significant after ~500M parameters.
```

**Takeaway:** Performance scales sublinearly - doubling parameters doesn't double performance. The sweet spot for quality/efficiency balance is around 100-500M parameters.

---

## ğŸ’» Quick Start Commands

### Running the Training Pipeline

```bash
# Stage A: Thinker Pretraining
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
python train_text.py --config configs/thinker_tiny.json

# Trains Thinker on text corpus
# Output: checkpoints/thinker_tiny/
# Expected: Perplexity < 30

# Stage B: Audio Encoder (ASR)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
python train_audio_enc.py --config configs/audio_enc_tiny.json

# Trains Audio Encoder for speech recognition
# Output: checkpoints/audio_encoder_tiny/
# Expected: WER < 30%

# Stage C: Vision Encoder
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
python train_vision.py --config configs/vision_tiny.json

# Trains Vision Encoder for image understanding
# Output: checkpoints/vision_encoder_tiny/
# Expected: Accuracy > 70%

# Stage D: Talker + RVQ Codec
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
python train_talker.py --config configs/talker_tiny.json

# Trains RVQ codec and Talker for speech generation
# Output: checkpoints/rvq_codec/ + checkpoints/talker_tiny/
# Expected: Intelligible speech output

# Stage E: Multimodal SFT
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
python sft_omni.py --config configs/omni_sft_tiny.json \
  --thinker checkpoints/thinker_tiny/final.pt \
  --audio_encoder checkpoints/audio_encoder_tiny/final.pt \
  --vision_encoder checkpoints/vision_encoder_tiny/final.pt \
  --talker checkpoints/talker_tiny/final.pt

# Trains all components jointly for multimodal understanding
# Output: checkpoints/omni_final/
# Expected: Successful multimodal Q&A

Complete! Î¼Omni is ready for inference! ğŸ‰
```

---

## ğŸ”„ Training Dependencies

### Stage Dependency Graph

```
Independent (Can run in parallel):
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Stage A â”‚     â”‚  Stage B     â”‚     â”‚  Stage C       â”‚
â”‚(Thinker)â”‚     â”‚(Audio Encoder)â”‚     â”‚(Vision Encoder)â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚                 â”‚                      â”‚
     â”‚                 â”‚                      â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â†“
             All feed into Stage E

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Stage D (Part 1) â”‚ â† Independent
â”‚ (RVQ Codec)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Stage D (Part 2) â”‚ â† Depends on Part 1
â”‚ (Talker)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Feeds into Stage E

Sequential (Must run in order):
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Stage E depends on ALL previous stages:
- Needs trained Thinker (from A)
- Needs trained Audio Encoder (from B)
- Needs trained Vision Encoder (from C)
- Needs trained Talker (from D)

Optimization:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Parallel strategy (if you have multiple GPUs):
- GPU 1: Stage A (8-12h)
- GPU 2: Stage B (6-10h)
- GPU 3: Stage C (4-8h)
- GPU 4: Stage D (10-15h)

Then GPU 1: Stage E (6-12h)

Total wall-clock time: ~25 hours instead of 50!
```

---

## ğŸ’¡ Key Takeaways

âœ… **5-stage pipeline** ensures stable, modular training  
âœ… **Independent stages** (A, B, C, D-part1) can run in parallel  
âœ… **Stage E** integrates all components for multimodal understanding  
âœ… **~40-60 hours total** on single 12GB GPU  
âœ… **Small datasets** (<5GB each) make it accessible  
âœ… **Modular design** enables easy debugging and experimentation  
âœ… **Progressive learning** from simple (text) to complex (multimodal)  
âœ… **Efficient** through gradient accumulation, FP16, and checkpointing

---

## ğŸ“ Self-Check Questions

1. Why does Î¼Omni use 5 separate training stages instead of training everything together?
2. Which stages can be run in parallel and why?
3. What is the purpose of Stage A and why must it come first conceptually?
4. How does modular training help with debugging?
5. What is the total estimated training time on a single 12GB GPU?

<details>
<summary>ğŸ“ Click to see answers</summary>

1. Separate stages avoid gradient conflicts, enable focused learning, simplify debugging, and allow modular development. Each component learns its specialized task before multimodal integration
2. Stages A, B, C, and D-part1 can run in parallel because they train independent components with no dependencies on each other. Only Stage E requires all previous stages to be complete
3. Stage A trains the Thinker (core LLM) on text. It must conceptually come first because language understanding is the foundation - the Thinker needs to understand text before it can process multimodal inputs
4. Modular training means if a stage fails, you can identify and fix only that component, then retrain just that stage and subsequent dependent stages. No need to retrain the entire pipeline
5. Approximately 40-60 hours total: Stage A (8-12h) + Stage B (6-10h) + Stage C (4-8h) + Stage D (10-15h) + Stage E (6-12h)
</details>

---

[Continue to Chapter 27: Stage A - Thinker Pretraining â†’](27-stage-a-thinker.md)

**Chapter Progress:** Training Pipeline â—â—‹â—‹â—‹â—‹â—‹ (1/6 complete)

---

## ğŸ“Š Training Summary

| Stage | Model | Task | Loss Function | Key Metric |
|-------|-------|------|---------------|------------|
| **A** | Thinker | Language Modeling | Cross-Entropy | Perplexity |
| **B** | Audio Encoder | ASR | CTC | WER |
| **C** | Vision Encoder | Image Understanding | Cross-Entropy | Accuracy |
| **D** | Talker + RVQ | Speech Generation | Cross-Entropy + MSE | Reconstruction |
| **E** | All (Joint) | Multimodal | Cross-Entropy | Mixed Accuracy |

## ğŸ¯ Training Strategy

### Modularity
- Each stage trains independently
- Debug issues in isolation
- Parallel development possible

### Efficiency
- Small datasets (<5GB per modality)
- Fits 12GB GPU with gradient accumulation
- Uses mixed precision (FP16)
- Gradient checkpointing for memory

### Progressive Learning
- Start with individual modalities
- End with joint understanding
- Specialized encoders preserved

## ğŸ’» Quick Start

```bash
# Stage A
python train_text.py --config configs/thinker_tiny.json

# Stage B
python train_audio_enc.py --config configs/audio_enc_tiny.json

# Stage C
python train_vision.py --config configs/vision_tiny.json

# Stage D
python train_talker.py --config configs/talker_tiny.json

# Stage E
python sft_omni.py --config configs/omni_sft_tiny.json
```

## ğŸ’¡ Key Takeaways

âœ… **5 independent stages** (modular design)  
âœ… **~40-60 hours total** training time (12GB GPU)  
âœ… **Small datasets** (<5GB each)  
âœ… **Progressive learning** (specialized â†’ joint)

---

[Back to Index](00-INDEX.md)

